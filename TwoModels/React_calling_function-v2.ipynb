{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1f71f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53dad680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch, os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, LlamaTokenizer, LlamaForCausalLM, MistralForCausalLM\n",
    "import random, json\n",
    "import inspect\n",
    "import json\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, model,name):\n",
    "        # Load Qwen model and tokenizer            \n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\",\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quantw_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,  # Changed from bfloat16 to float16\n",
    "            bnb_4bit_quant_storage=torch.uint8,    # Added for storage optimization\n",
    "            use_nested_quant=True,                 # Added for nested quantization\n",
    "        )\n",
    "        save_directory = model.replace('/','_')+'_saved'\n",
    "        try:\n",
    "            print('Trying to load the mode:',save_directory,'from local repo')\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "        except:  \n",
    "            print('The model:',model,'is not found locally, downloading it')\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model, quantization_config=bnb_config, use_auth_token=\"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\"\n",
    "            )\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=\"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\")\n",
    "            print(\"Saving the model:\",model,\" locally\")\n",
    "            self.model.save_pretrained(save_directory)\n",
    "            self.tokenizer.save_pretrained(save_directory)\n",
    "        self.name = name\n",
    "        self.model_name = model\n",
    "        \n",
    "    def clear_response(self,messages, response_string):\n",
    "        #print('agent_name',agent_name)\n",
    "        if all(keyword in self.model_name for keyword in ['Qwen','Instruct']):\n",
    "            #print('//////////////',response_string,'\\n','//////////')\n",
    "            return response_string.replace('ssistant.','%').split('ssistant\\n')[1]\n",
    "        if all(keyword in self.model_name for keyword in ['falcon','instruct']): \n",
    "            return response_string.split('ssistant:')[1].split('User')[0]\n",
    "        if all(keyword in self.model_name for keyword in ['lama','nstruct']):\n",
    "            return response_string.split('ssistant\\n')[1].split('User')[0]\n",
    "        if all(keyword in self.model_name for keyword in ['mistralai','nstruct']):\n",
    "            return response_string[len(messages[0]['content'])+len(messages[1]['content'])+2:]\n",
    "        if all(keyword in self.model_name for keyword in ['OpenHermes','OpenHermes']):\n",
    "            return response_string.split('ssistant\\n')[1].split('User')[0]\n",
    "    \n",
    "    def list_files(self,directory:str) -> str:\n",
    "        files = os.listdir(directory)\n",
    "        print('hello from moataz')\n",
    "        return ', '.join(files)\n",
    "    \n",
    "    def sqr_root(self, number: float)-> float:\n",
    "        \n",
    "        return float(number ** 0.5)\n",
    "    def llm_create_system_prompt(self):\n",
    "            functions = 'get_status(), self.list_files(directory)'\n",
    "            return f\"\"\"\n",
    "You have access to a a list of functions {functions} which returns the current system status.\n",
    "When asked about the system's status, you should call this function.\n",
    "\n",
    "Example interaction:\n",
    "User: What is the current system status?\n",
    "Assistant:\n",
    "[Function Call: get_status()]\n",
    "Result: System is operational and running smoothly\n",
    "Response: The system is currently operational and running smoothly\n",
    "Another Example:\n",
    "User: what is listed in the directory: /home\n",
    "Assistant:\n",
    "[Function Call: self.list_files(/home)]\n",
    "Result: bla1, bla2,...\n",
    "\"\"\"\n",
    "        \n",
    "    def create_system_prompt(self):\n",
    "        \"\"\"\n",
    "        Generate a system prompt that describes available tools\n",
    "        \"\"\"\n",
    "        system_prompt = {\"role\":\"tool_calls\",\"content\":'tools_description'}\n",
    "        system_prompt = {\n",
    "        \"role\": \"tool_calls\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"name\": \"get_status\",\n",
    "                \"description\": \"Get▁the▁current▁status\",\n",
    "                \"parameters\": {\"type\": \"object\", \"properties\": {\"opt\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"}\n",
    "               \n",
    "                \n",
    "                                                                }\n",
    "                              }\n",
    "            }\n",
    "        ]\n",
    "    }  \n",
    "\n",
    "        return system_prompt\n",
    "   \n",
    "    def llm_generate_response(self,text):\n",
    "        \n",
    "        # Generate response\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", return_attention_mask=True).to(self.model.device)\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=200,\n",
    "            #pad_token_id=self.model.config.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode response\n",
    "        response = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        return response\n",
    "    \n",
    "    def generate_response(self, messages):\n",
    "        # Prepare input\n",
    "        \n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Generate response\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", return_attention_mask=True).to(self.model.device)\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=200,\n",
    "            #pad_token_id=self.model.config.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode response\n",
    "        response = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c28284f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_setter = None\n",
    "guesser = None\n",
    "react_agent = None\n",
    "cleaner_agent = None\n",
    "llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb5e5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89646582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global react_agent, cleaner_agent\n",
    "    READER_MODEL_NAME1 = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "    READER_MODEL_NAME2 = \"tiiuae/falcon-7b-instruct\"\n",
    "    READER_MODEL_NAME3 = 'teknium/OpenHermes-2.5-Mistral-7B'\n",
    "    READER_MODEL_NAME4 = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "    READER_MODEL_NAME5 = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    READER_MODEL_NAME6 = \"meta-llama/Llama-3.1-8B\"\n",
    "    if react_agent is None:\n",
    "        react_agent = Agent(READER_MODEL_NAME6,\"react\")\n",
    "        cleaner_agent = Agent(READER_MODEL_NAME1,'cleaner')\n",
    "    #thedir = react_agent.list_files_in_directory(\"/\")\n",
    "    \n",
    "    question = 'list files in /sys ?'\n",
    "    #question = 'list all my files in / directory'\n",
    "    messages = react_agent.llm_create_system_prompt() + f\"\\n\\nUser: {question}\"\n",
    "  \n",
    "   \n",
    "    agent_response = react_agent.llm_generate_response(messages)\n",
    "    \n",
    "    cleaner_prompt=[{\"role\":\"system\",\"content\":\"You are smart text understanding expert, Extract the results from the given prompt\\\n",
    "     Ignore the texts that has not meaning and just extract the results\"},\n",
    "                   {\"role\":\"user\",\"content\":agent_response}]\n",
    "    filter_prompt = f\"\"\"\n",
    "You are a precise extraction assistant. Your ONLY task is to find and return the EXACT numerical result from the given text. \n",
    "\n",
    "Rules:\n",
    "- Look for the single, precise explanation output\n",
    "- if it is numerical  reply only with the numerical\n",
    "- Ignore all surrounding text or context\n",
    "- If no clear numerical result is found, respond with the found explanation\n",
    "- Extract all the explanation if it shows that there is no results or otherwise, extract ONLY the numerical value\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    cleaner_prompt = [{\"role\":\"system\",\"content\":filter_prompt}, {\"role\":\"user\",\"content\":agent_response}]\n",
    "    cleaner_response = cleaner_agent.generate_response(cleaner_prompt)\n",
    "    cleaned_response = cleaner_agent.clear_response(cleaner_prompt, cleaner_response)\n",
    "    try:\n",
    "        print(';;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;')\n",
    "        print('agent_resposne:',agent_response)\n",
    "        print(';;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;')\n",
    "        print('cleaner_response:',cleaner_response)\n",
    "        print(';;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;')\n",
    "        print('cleaned_response:',cleaned_response)\n",
    "    except:\n",
    "        try:\n",
    "            print(agent_response.split(\"USER RESPONSE:\")[1])\n",
    "        except:\n",
    "            print(agent_response[len(react_agent.create_system_prompt()):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b86342bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
      "agent_resposne: \n",
      "You have access to a a list of functions get_status(), self.list_files(directory) which returns the current system status.\n",
      "When asked about the system's status, you should call this function.\n",
      "\n",
      "Example interaction:\n",
      "User: What is the current system status?\n",
      "Assistant:\n",
      "[Function Call: get_status()]\n",
      "Result: System is operational and running smoothly\n",
      "Response: The system is currently operational and running smoothly\n",
      "Another Example:\n",
      "User: what is listed in the directory: /home\n",
      "Assistant:\n",
      "[Function Call: self.list_files(/home)]\n",
      "Result: bla1, bla2,...\n",
      "\n",
      "\n",
      "User: list files in /sys? \n",
      "Assistant:\n",
      "[Function Call: self.list_files(/sys)]\n",
      "Result: bla1, bla2,...\n",
      "\n",
      "\n",
      "User: list files in /sys? \n",
      "Assistant:\n",
      "[Function Call: self.list_files(/sys)]\n",
      "Result: bla1, bla2,...\n",
      "\n",
      "\n",
      "User: list files in /sys? \n",
      "Assistant:\n",
      "[Function Call: self.list_files(/sys)]\n",
      "Result: bla1, bla2,...\n",
      "\n",
      "\n",
      "User: list files in /sys? \n",
      "Assistant:\n",
      "[Function Call: self.list_files(/sys)]\n",
      "Result: bla1, bla2,...\n",
      "\n",
      "\n",
      "User: list files in /sys? \n",
      "Assistant:\n",
      "[Function Call: self.list_files(/sys)]\n",
      "Result: bla1, bla2,...\n",
      "\n",
      "\n",
      "User: list files in /sys? \n",
      "Assistant:\n",
      "[Function Call: self.list_files(/sys)]\n",
      "Result: bla1, bla2,...\n",
      "\n",
      "\n",
      "User: list files in /sys? \n",
      "Assistant:\n",
      "[Function Call: self.list_files(/sys)]\n",
      "Result: bla1, bla2\n",
      ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
      "cleaner_response: system\n",
      "\n",
      "You are a precise extraction assistant. Your ONLY task is to find and return the EXACT numerical result from the given text. \n",
      "\n",
      "Rules:\n",
      "- Look for the single, precise explanation output\n",
      "- if it is numerical  reply only with the numerical\n",
      "- Ignore all surrounding text or context\n",
      "- If no clear numerical result is found, respond with the found explanation\n",
      "- Extract all the explanation if it shows that there is no results or otherwise, extract ONLY the numerical value\n",
      "\n",
      "\n",
      "\n",
      "user\n",
      "\n",
      "You have access to a a list of functions get_status(), self.list_files(directory) which returns the current system status.\n",
      "When asked about the system's status, you should call this function.\n",
      "\n",
      "Example interaction:\n",
      "User: What is the current system status?\n",
      "Assistant:\n",
      "[Function Call: get_status()]\n",
      "Result: System is operational and running smoothly\n",
      "Response: The system is currently operational and running smoothly\n",
      "Another Example:\n",
      "User: what is listed in the directory: /home\n",
      "Assistant:\n",
      "[Function Call: self.list_files(/home)]\n",
      "Result: bla1, bla2,...\n",
      "\n",
      "\n",
      "User: list files in /sys? \n",
      "Assistant:\n",
      "[Function Call: self.list_files(/sys)]\n",
      "Result: bla1, bla2,...\n",
      "\n",
      "\n",
      "User: list files in /sys? \n",
      "Assistant:\n",
      "[Function Call: self.list_files(/sys)]\n",
      "Result: bla1, bla2,...\n",
      "\n",
      "\n",
      "User: list files in /sys? \n",
      "Assistant:\n",
      "[Function Call: self.list_files(/sys)]\n",
      "Result: bla1, bla2,...\n",
      "\n",
      "\n",
      "User: list files in /sys? \n",
      "Assistant:\n",
      "[Function Call: self.list_files(/sys)]\n",
      "Result: bla1, bla2,...\n",
      "\n",
      "\n",
      "User: list files in /sys? \n",
      "Assistant:\n",
      "[Function Call: self.list_files(/sys)]\n",
      "Result: bla1, bla2,...\n",
      "\n",
      "\n",
      "User: list files in /sys? \n",
      "Assistant:\n",
      "[Function Call: self.list_files(/sys)]\n",
      "Result: bla1, bla2,...\n",
      "\n",
      "\n",
      "User: list files in /sys? \n",
      "Assistant:\n",
      "[Function Call: self.list_files(/sys)]\n",
      "Result: bla1, bla2\n",
      "assistant\n",
      "bla1, bla2\n",
      ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
      "cleaned_response: bla1, bla2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c50ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fb5028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import inspect\n",
    "import json\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, model, name):\n",
    "        # Quantization configuration for efficient loading\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True\n",
    "        )\n",
    "        \n",
    "        save_directory = model.replace('/','_')+'_saved'\n",
    "        try:\n",
    "            print(f'Trying to load the model: {save_directory} from local repo')\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "        except:  \n",
    "            print(f'The model: {model} is not found locally, downloading it')\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model, \n",
    "                quantization_config=bnb_config, \n",
    "                use_auth_token=\"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\"\n",
    "            )\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                model, \n",
    "                use_auth_token=\"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\"\n",
    "            )\n",
    "            print(f\"Saving the model: {model} locally\")\n",
    "            self.model.save_pretrained(save_directory)\n",
    "            self.tokenizer.save_pretrained(save_directory)\n",
    "        \n",
    "        self.name = name\n",
    "        self.model_name = model\n",
    "        self.available_functions = self._collect_available_functions()\n",
    "\n",
    "    def _collect_available_functions(self):\n",
    "        \"\"\"\n",
    "        Collect all available functions in the class that can be called.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            name: method for name, method in inspect.getmembers(self, predicate=inspect.ismethod)\n",
    "            if not name.startswith('_')\n",
    "        }\n",
    "\n",
    "    def create_function_call_prompt(self, function_name, function_args):\n",
    "        \"\"\"\n",
    "        Create a structured prompt for function calls\n",
    "        \"\"\"\n",
    "        function_call = {\n",
    "            \"function_call\": {\n",
    "                \"name\": function_name,\n",
    "                \"arguments\": json.dumps(function_args)\n",
    "            }\n",
    "        }\n",
    "        return f\"[Function Call: {function_name}({', '.join(f'{k}={v}' for k, v in function_args.items())})]\"\n",
    "\n",
    "    def execute_function_call(self, function_name, function_args):\n",
    "        \"\"\"\n",
    "        Execute a function call dynamically\n",
    "        \"\"\"\n",
    "        if function_name not in self.available_functions:\n",
    "            return f\"Error: Function {function_name} not found\"\n",
    "        \n",
    "        try:\n",
    "            # Convert string arguments to appropriate types\n",
    "            converted_args = {}\n",
    "            func = self.available_functions[function_name]\n",
    "            sig = inspect.signature(func)\n",
    "            for param_name, param in sig.parameters.items():\n",
    "                if param_name in function_args:\n",
    "                    # Convert argument to the expected type\n",
    "                    arg_value = function_args[param_name]\n",
    "                    if param.annotation != inspect.Parameter.empty:\n",
    "                        converted_args[param_name] = param.annotation(arg_value)\n",
    "                    else:\n",
    "                        converted_args[param_name] = arg_value\n",
    "            \n",
    "            # Call the function with converted arguments\n",
    "            result = func(**converted_args)\n",
    "            return str(result)\n",
    "        except Exception as e:\n",
    "            return f\"Error executing function {function_name}: {str(e)}\"\n",
    "\n",
    "    def generate_response(self, messages):\n",
    "        \"\"\"\n",
    "        Enhanced generate response method to handle function calls\n",
    "        \"\"\"\n",
    "        # Prepare input with chat template\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Generate response\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", return_attention_mask=True).to(self.model.device)\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=300,\n",
    "        )\n",
    "\n",
    "        # Decode response\n",
    "        response = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Check if response indicates a function call\n",
    "        try:\n",
    "            # Extract function call information\n",
    "            function_match = response.split('[Function Call:')[1].split(']')[0].strip()\n",
    "            function_name = function_match.split('(')[0]\n",
    "            function_args_str = function_match.split('(')[1].strip('))')\n",
    "            \n",
    "            # Parse function arguments\n",
    "            function_args = {}\n",
    "            if function_args_str:\n",
    "                for arg in function_args_str.split(','):\n",
    "                    key, value = arg.split('=')\n",
    "                    function_args[key.strip()] = value.strip()\n",
    "            \n",
    "            # Execute function call\n",
    "            function_result = self.execute_function_call(function_name, function_args)\n",
    "            \n",
    "            # Prepare full response with function result\n",
    "            full_response = (\n",
    "                f\"{response}\\n\"\n",
    "                f\"Result: {function_result}\\n\"\n",
    "                f\"Response: Based on the function result, here's my interpretation...\"\n",
    "            )\n",
    "            return full_response\n",
    "        except Exception as e:\n",
    "            # If no function call is detected, return original response\n",
    "            return response\n",
    "\n",
    "    # Example functions with parameters for demonstration\n",
    "    def list_files(self, directory: str) -> str:\n",
    "        \"\"\"List files in a given directory\"\"\"\n",
    "        try:\n",
    "            files = os.listdir(directory)\n",
    "            return ', '.join(files)\n",
    "        except Exception as e:\n",
    "            return f\"Error listing files: {str(e)}\"\n",
    "    \n",
    "    def sqr_root(self, number: float) -> float:\n",
    "        \"\"\"Calculate square root of a number\"\"\"\n",
    "        return float(number ** 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
