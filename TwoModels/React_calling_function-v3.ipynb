{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1f71f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53dad680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch, os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, LlamaTokenizer, LlamaForCausalLM, MistralForCausalLM\n",
    "import random, json\n",
    "import inspect\n",
    "import json\n",
    "from typing import Dict, Any, Optional, Callable, List\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, model,name):\n",
    "        # Load Qwen model and tokenizer            \n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\",\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quantw_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,  # Changed from bfloat16 to float16\n",
    "            bnb_4bit_quant_storage=torch.uint8,    # Added for storage optimization\n",
    "            use_nested_quant=True,                 # Added for nested quantization\n",
    "        )\n",
    "        save_directory = model.replace('/','_')+'_saved_response'\n",
    "        try:\n",
    "            print('Trying to load the mode:',save_directory,'from local repo')\n",
    "\n",
    "            #self.model = pipeline.load_from_pretrained(save_directory)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "        except:  \n",
    "            print('The model:',model,'is not found locally, downloading it')\n",
    "            #self.model = pipeline(\n",
    "            #    \"text-generation\",\n",
    "            #    model=model,\n",
    "            #    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            #    device_map=\"auto\",\n",
    "            #    token=\"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\"\n",
    "            #)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model, quantization_config=bnb_config, token=\"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\"\n",
    "            )\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model, token=\"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\")\n",
    "            print(\"Saving the model:\",model,\" locally\")\n",
    "            #self.model.save_pretrained(save_directory)\n",
    "            self.model.save_pretrained(save_directory)\n",
    "            self.tokenizer.save_pretrained(save_directory)\n",
    "        self.name = name\n",
    "        self.model_name = model\n",
    "        self.system_message = \"\"\n",
    "        self.tools = []\n",
    "        \n",
    "    def clear_response(self,messages, response_string):\n",
    "        #print('agent_name',agent_name)\n",
    "        if all(keyword in self.model_name for keyword in ['Qwen','Instruct']):\n",
    "            #print('//////////////',response_string,'\\n','//////////')\n",
    "            return response_string.replace('ssistant.','%').split('ssistant\\n')[1]\n",
    "        if all(keyword in self.model_name for keyword in ['falcon','instruct']): \n",
    "            return response_string.split('ssistant:')[1].split('User')[0]\n",
    "        if all(keyword in self.model_name for keyword in ['lama','nstruct']):\n",
    "            return response_string.split('ssistant\\n')[1].split('User')[0]\n",
    "        if all(keyword in self.model_name for keyword in ['mistralai','nstruct']):\n",
    "            return response_string[len(messages[0]['content'])+len(messages[1]['content'])+2:]\n",
    "        if all(keyword in self.model_name for keyword in ['OpenHermes','OpenHermes']):\n",
    "            return response_string.split('ssistant\\n')[1].split('User')[0]\n",
    "    \n",
    "    \n",
    "    def sqr_root(self, number: float)-> float:\n",
    "        \n",
    "        return float(number ** 0.5)\n",
    "    def llm_create_system_prompt(self,prompt):\n",
    "            \n",
    "            return [\n",
    "                dict({\"role\": \"system\", \"content\": self.system_message}),\n",
    "                dict({\"role\": \"user\", \"content\": prompt}),\n",
    "            ]\n",
    "        \n",
    "            functions = 'get_status(), self.list_files(directory)'\n",
    "            '''\n",
    "            return f\"\"\"\n",
    "You have access to a a list of functions {functions} which returns the current system status.\n",
    "When asked about the system's status, you should call this function.\n",
    "\n",
    "Example interaction:\n",
    "User: What is the current system status?\n",
    "Assistant:\n",
    "[Function Call: get_status()]\n",
    "Result: System is operational and running smoothly\n",
    "Response: The system is currently operational and running smoothly\n",
    "Another Example:\n",
    "User: what is listed in the directory: /home`\n",
    "Assistant:\n",
    "[Function Call: self.list_files(/home)]\n",
    "Result: bla1, bla2,...\n",
    "\"\"\"\n",
    "        '''\n",
    "        \n",
    "    def create_system_prompt(self):\n",
    "        \"\"\"\n",
    "        Generate a system prompt that describes available tools\n",
    "        \"\"\"\n",
    "        system_prompt = {\"role\":\"tool_calls\",\"content\":'tools_description'}\n",
    "        system_prompt = {\n",
    "        \"role\": \"tool_calls\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"name\": \"get_status\",\n",
    "                \"description\": \"Get▁the▁current▁status\",\n",
    "                \"parameters\": {\"type\": \"object\", \"properties\": {\"opt\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"}\n",
    "               \n",
    "                \n",
    "                                                                }\n",
    "                              }\n",
    "            }\n",
    "        ]\n",
    "    }  \n",
    "\n",
    "        return system_prompt\n",
    "    \n",
    "        \n",
    "    def get_tool_schema(self,func: Callable) -> dict:\n",
    "        \"\"\"\n",
    "        Generate a JSON schema for a tool function.\n",
    "\n",
    "        Args:\n",
    "            func (Callable): The function to generate a schema for.\n",
    "\n",
    "        Returns:\n",
    "            dict: A JSON schema representing the function's parameters.\n",
    "        \"\"\"\n",
    "        import inspect\n",
    "        signature = inspect.signature(func)\n",
    "        parameters = {}\n",
    "\n",
    "        for name, param in signature.parameters.items():\n",
    "            parameters[name] = {\n",
    "                \"type\": \"string\",  # Assume string type for simplicity\n",
    "                \"description\": f\"Parameter {name}\"\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"description\": func.__doc__.split('\\n')[0] if func.__doc__ else \"\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": parameters\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def llm_generate_response(self,prompt):\n",
    "        \n",
    "        # Generate response\n",
    "        messages =  self.llm_create_system_prompt(prompt)\n",
    "        schema_tools = []\n",
    "        \n",
    "        for tool in self.tools:\n",
    "            schema_tools.append(self.get_tool_schema(tool))\n",
    "            \n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tools= self.tools,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Generate response\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", return_attention_mask=True).to(self.model.device)\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=384,\n",
    "            #pad_token_id=self.model.config.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode response\n",
    "        response = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        # Decode response\n",
    "        \n",
    "        #outputs = pipeline(\n",
    "        #    messages,\n",
    "        #    max_new_tokens=256,\n",
    "        #)\n",
    "        return response\n",
    "    \n",
    "    def generate_response(self, messages):\n",
    "        # Prepare input\n",
    "        \n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Generate response\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", return_attention_mask=True).to(self.model.device)\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=384,\n",
    "            #pad_token_id=self.model.config.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode response\n",
    "        response = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c28284f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_setter = None\n",
    "guesser = None\n",
    "react_agent = None\n",
    "cleaner_agent = None\n",
    "llm = None\n",
    "rephrase_agent = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c56279a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(directory:str) -> str:\n",
    "    \"\"\"\n",
    "        List files in the specified directory.\n",
    "\n",
    "        Args:\n",
    "            directory (str): The path to the directory to list files from.\n",
    "\n",
    "        Returns:\n",
    "            str: A comma-separated string of files in the directory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        files = os.listdir(directory)\n",
    "        return ', '.join(files)\n",
    "    except Exception as e:\n",
    "        return f\"Error listing files: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bb5e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_status()->str:\n",
    "    \"\"\"\n",
    "    Get the current status\n",
    "    \n",
    "    Args:\n",
    "        {}\n",
    "        \n",
    "    Returns:\n",
    "        The current status as a string.\n",
    "    \"\"\"\n",
    "    \n",
    "    return \"my status is great\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de280b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match <re.Match object; span=(0, 55), match='{\"name\": \"list_files\", \"arguments\": {\"directory\":> {\"name\": \"list_files\", \"arguments\": {\"directory\": \"/\"}}}dlk\n",
      "checking directory /\n",
      "output:bin, boot, dev, etc, home, lib, lib32, lib64, libx32, media, mnt, opt, proc, root, run, sbin, srv, sys, tmp, usr, var, .dockerenv, workspace, .singularity.d\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "def list_files(directory):\n",
    "    \"\"\"\n",
    "    Lists files in the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to the directory\n",
    "    \n",
    "    Returns:\n",
    "        list: List of files in the directory\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print('checking directory',directory)\n",
    "        return ', '.join(os.listdir(directory))\n",
    "    except Exception as e:\n",
    "        return f\"Error listing files: {e}\"\n",
    "def clean_json(json_str):\n",
    "    \"\"\"\n",
    "    Clean the JSON string by removing trailing characters that might break parsing.\n",
    "    \n",
    "    Args:\n",
    "        json_str (str): The potentially malformed JSON string\n",
    "    \n",
    "    Returns:\n",
    "        str: A cleaned JSON string that can be parsed by json.loads()\n",
    "    \"\"\"\n",
    "    # Remove the 'function_call:' prefix and any surrounding text\n",
    "    json_str = re.sub(r'^.*?function_call:\\s*', '', json_str, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove escaped quotes and unescape the string\n",
    "    json_str = json_str.replace('\\\\\"', '\"').strip()\n",
    "    \n",
    "    # Remove any trailing characters after the JSON object\n",
    "    json_str = re.sub(r'}.*$', '}', json_str, flags=re.DOTALL)\n",
    "    return json_str\n",
    "\n",
    "def execute_function_call(llm_output):\n",
    "    \"\"\"\n",
    "    Extracts and executes the function call from LLM output.\n",
    "    \n",
    "    Args:\n",
    "        llm_output (str): The full LLM output string\n",
    "    \n",
    "    Returns:\n",
    "        Result of the function call or None if no valid function call\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use regex to find the JSON between function_call: and the next closing brace\n",
    "    llm_output = llm_output.replace('parameters','arguments')\n",
    "    match = re.search(r'\\s*({.*})', llm_output, re.DOTALL)\n",
    "    match = re.search(r'\\s*({(?:\\\\}|[^}])*?}?})(?:,|\\]|\\'|\\\")?', llm_output, re.DOTALL)\n",
    "    print('match',match, llm_output)\n",
    "    if match:\n",
    "        try:\n",
    "            # Extract the potential JSON string\n",
    "            potential_json = match.group(1)\n",
    "            # Clean the JSON string\n",
    "            cleaned_json = clean_json(potential_json)\n",
    "            # Parse the cleaned JSON\n",
    "            function_call = json.loads(potential_json)\n",
    "           \n",
    "            \n",
    "            # Get the function name and arguments\n",
    "            func_name = function_call.get('name')\n",
    "            args = function_call.get('arguments', {})\n",
    "\n",
    "            \n",
    "            # Dynamically call the function\n",
    "            if func_name == 'list_files':\n",
    "                return 'output:'+ list_files(args.get('directory'))\n",
    "            if func_name == 'get_status':\n",
    "                return 'output:'+get_status()\n",
    "            else:\n",
    "                return f\"Unknown function: {func_name}\"\n",
    "        \n",
    "        except json.JSONDecodeError as e:\n",
    "            return f\"JSON Parsing Error: {e}. Original string: {potential_json}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error processing function call: {e}\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "sample_output = '{\"name\": \"list_files\", \"parameters\": {\"directory\": \"/\"}}}dlk'\n",
    "result = execute_function_call(sample_output)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27379e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89646582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def react():\n",
    "    global react_agent, cleaner_agent, rephrase_agent,  prompt, system_message, tools\n",
    "    READER_MODEL_NAME1 = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "    READER_MODEL_NAME2 = \"tiiuae/falcon-7b-instruct\"\n",
    "    READER_MODEL_NAME3 = 'teknium/OpenHermes-2.5-Mistral-7B'\n",
    "    READER_MODEL_NAME4 = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "    READER_MODEL_NAME5 = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    READER_MODEL_NAME6 = \"meta-llama/Llama-3.1-8B\"\n",
    "    READER_MODEL_NAME7 = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    READER_MODEL_NAME8 = \"meta-llama/Meta-Llama-3.1-8b-Instruct\"\n",
    "    READER_MODEL_NAME9 = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    \n",
    "    if react_agent is None:\n",
    "        react_agent = Agent(READER_MODEL_NAME8,\"react\")\n",
    "        cleaner_agent = Agent(READER_MODEL_NAME9,'cleaner')\n",
    "        #rephrase_agent = Agent(READER_MODEL_NAME6,'rephrase')\n",
    "        \n",
    "    react_agent.system_message = system_message\n",
    "    react_agent.tools = tools\n",
    "    #thedir = react_agent.list_files_in_directory(\"/\")\n",
    "    \n",
    "    question = prompt\n",
    "    agent_response = react_agent.llm_generate_response(question)\n",
    "    print('####################################')\n",
    "    agent_response = list(agent_response.split(['assistant'][-1]))[-1]\n",
    "    print(agent_response)\n",
    "    print('####################################')\n",
    "    \n",
    "    cleaner_prompt=[{\"role\":\"system\",\"content\":\"You are smart text understanding expert, Extract the results from the given prompt\\\n",
    "     Ignore the texts that has not meaning and just extract the results\"},\n",
    "                   {\"role\":\"user\",\"content\":agent_response}]\n",
    "    filter_prompt = f\"\"\"\n",
    "You are a precise extraction of assistant: or Assistant: replies.\\\n",
    "Your ONLY task is to find and return the result from the given text. \n",
    "\n",
    "Rules:\n",
    "- Look for the single, precise explanation output\n",
    "- if it is numerical  reply only with the numerical\n",
    "- Ignore all surrounding text or context\n",
    "- If no clear numerical result is found, respond with the found explanation\n",
    "- If you find a list of dictionnaries or a list of json data after the assistant/Assistant words, \\\n",
    "then say the word 'Execute Functions:' and list them\n",
    "- Extract all the explanation if it shows that there is no results or otherwise, extract ONLY the numerical value\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    cleaner_prompt = [{\"role\":\"system\",\"content\":filter_prompt}, {\"role\":\"user\",\"content\":agent_response}]\n",
    "    #cleaner_response = cleaner_agent.generate_response(cleaner_prompt)\n",
    "    #cleaned_response = cleaner_agent.clear_response(cleaner_prompt, cleaner_response)\n",
    "    try:\n",
    "        #print(';;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;')\n",
    "        #print('agent_resposne:',type(agent_response),str(agent_response.split(['Assistant:'][-1])))\n",
    "        #print(';;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;')\n",
    "        #print('cleaner_response:',cleaner_response)\n",
    "        print(';;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;')\n",
    "        #print('cleaned_response:',cleaned_response)\n",
    "        print(';;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;')\n",
    "        function_call = execute_function_call(agent_response)\n",
    "        if 'output:' in function_call:\n",
    "            #prompt = 'the function is called and the output is:' + function_call + 'so please \\\n",
    "            #rephrase youre reply'\n",
    "            rephrase_filter = \" you are expert in english language you rephrase any text you professionally.\\\n",
    "                    be very brief and reply by rephrasing the given text and starty\"\n",
    "            \n",
    "            rephrase_text = \"rephrase in a bief way and do not add any infomration from your side \\\n",
    "            : After cheking the outside environment, the  reply is:\"+function_call\n",
    "            cleaner_prompt = [{\"role\":\"system\",\"content\":rephrase_filter}, {\"role\":\"user\",\"content\":rephrase_text}]\n",
    "            print('hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh')\n",
    "            rephrase_response = cleaner_agent.generate_response(cleaner_prompt)\n",
    "            cleaned_response = cleaner_agent.clear_response(cleaner_prompt, rephrase_response)\n",
    "            print(cleaned_response)\n",
    "            \n",
    "            \n",
    "    except:\n",
    "        print('all is not running')\n",
    "      \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ada0a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b86342bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['torch_dtype', 'device_map', 'bnb_4bit_quantw_type', 'use_nested_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load the mode: meta-llama_Meta-Llama-3.1-8b-Instruct_saved_response from local repo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3920903bc3844a00b5d855967f72844c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['torch_dtype', 'device_map', 'bnb_4bit_quantw_type', 'use_nested_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load the mode: meta-llama_Llama-3.2-1B-Instruct_saved_response from local repo\n"
     ]
    },
    {
     "ename": "TypeHintParsingException",
     "evalue": "Argument directory is missing a type hint in function list_files",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeHintParsingException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlist the files in the / \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#prompt = \"get status\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#prompt = \"what files found in the /bin ?\"\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mreact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 23\u001b[0m, in \u001b[0;36mreact\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#thedir = react_agent.list_files_in_directory(\"/\")\u001b[39;00m\n\u001b[1;32m     22\u001b[0m question \u001b[38;5;241m=\u001b[39m prompt\n\u001b[0;32m---> 23\u001b[0m agent_response \u001b[38;5;241m=\u001b[39m \u001b[43mreact_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_generate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m####################################\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m agent_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(agent_response\u001b[38;5;241m.\u001b[39msplit([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[9], line 163\u001b[0m, in \u001b[0;36mAgent.llm_generate_response\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools:\n\u001b[1;32m    161\u001b[0m     schema_tools\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_tool_schema(tool))\n\u001b[0;32m--> 163\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    168\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[1;32m    171\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1812\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1810\u001b[0m     tool_schemas\u001b[38;5;241m.\u001b[39mappend(tool)\n\u001b[1;32m   1811\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m isfunction(tool):\n\u001b[0;32m-> 1812\u001b[0m     tool_schemas\u001b[38;5;241m.\u001b[39mappend(\u001b[43mget_json_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1814\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTools should either be a JSON schema, or a callable function with type hints \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand a docstring suitable for auto-conversion to a schema.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1817\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/chat_template_utils.py:297\u001b[0m, in \u001b[0;36mget_json_schema\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    294\u001b[0m doc \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    295\u001b[0m main_doc, param_descriptions, return_doc \u001b[38;5;241m=\u001b[39m parse_google_format_docstring(doc)\n\u001b[0;32m--> 297\u001b[0m json_schema \u001b[38;5;241m=\u001b[39m \u001b[43m_convert_type_hints_to_json_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (return_dict \u001b[38;5;241m:=\u001b[39m json_schema[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_doc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# We allow a missing return docstring since most templates ignore it\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/chat_template_utils.py:134\u001b[0m, in \u001b[0;36m_convert_type_hints_to_json_schema\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_name, param \u001b[38;5;129;01min\u001b[39;00m signature\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mannotation \u001b[38;5;241m==\u001b[39m inspect\u001b[38;5;241m.\u001b[39mParameter\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m--> 134\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TypeHintParsingException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is missing a type hint in function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdefault \u001b[38;5;241m==\u001b[39m inspect\u001b[38;5;241m.\u001b[39mParameter\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m    136\u001b[0m         required\u001b[38;5;241m.\u001b[39mappend(param_name)\n",
      "\u001b[0;31mTypeHintParsingException\u001b[0m: Argument directory is missing a type hint in function list_files"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tools=[]\n",
    "    tools = [get_status, list_files]\n",
    "    system_message = \"you are a bot .\\\n",
    "    start youre reply with Assistant:. if the question needs you to run a function, then only say the following:\\\n",
    "    function_call: and add the function and arguments in a json format and end your reply. \\\n",
    "    remember not to state any other function that are not needed.\\\n",
    "    and remember to not to run the function. \\\n",
    "    if there is no function is needed to call, then be brief in your reply and only reply directly\\\n",
    "  \"\n",
    "    prompt = \"list the files in the / \"\n",
    "    #prompt = \"get status\"\n",
    "    #prompt = \"what files found in the /bin ?\"\n",
    "    react()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c50ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fb5028",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
