{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92462436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch, os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, LlamaTokenizer, LlamaForCausalLM, MistralForCausalLM\n",
    "import random, json\n",
    "import inspect\n",
    "import json\n",
    "from typing import Dict, Any, Optional, Callable, List\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, model,name):\n",
    "        # Load Qwen model and tokenizer            \n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\",\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quantw_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,  # Changed from bfloat16 to float16\n",
    "            bnb_4bit_quant_storage=torch.uint8,    # Added for storage optimization\n",
    "            use_nested_quant=True,                 # Added for nested quantization\n",
    "        )\n",
    "        save_directory = model.replace('/','_')+'_saved_response'\n",
    "        try:\n",
    "            print('Trying to load the mode:',save_directory,'from local repo')\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "        except:  \n",
    "            print('The model:',model,'is not found locally, downloading it')\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model, quantization_config=bnb_config, token=\"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\"\n",
    "            )\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model, token=\"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\")\n",
    "            print(\"Saving the model:\",model,\" locally\")\n",
    "            self.model.save_pretrained(save_directory)\n",
    "            self.tokenizer.save_pretrained(save_directory)\n",
    "        self.name = name\n",
    "        self.model_name = model\n",
    "        self.system_message = \"\"\n",
    "        self.tools = []\n",
    "        \n",
    "    def clear_response(self,messages, response_string):\n",
    "        #print('agent_name',agent_name)\n",
    "        if all(keyword in self.model_name for keyword in ['Qwen','Instruct']):\n",
    "            #print('//////////////',response_string,'\\n','//////////')\n",
    "            return response_string.replace('ssistant.','%').split('ssistant\\n')[1]\n",
    "        if all(keyword in self.model_name for keyword in ['falcon','instruct']): \n",
    "            return response_string.split('ssistant:')[1].split('User')[0]\n",
    "        if all(keyword in self.model_name for keyword in ['lama','nstruct']):\n",
    "            return response_string.split('ssistant\\n')[1].split('User')[0]\n",
    "        if all(keyword in self.model_name for keyword in ['mistralai','nstruct']):\n",
    "            return response_string[len(messages[0]['content'])+len(messages[1]['content'])+2:]\n",
    "        if all(keyword in self.model_name for keyword in ['OpenHermes','OpenHermes']):\n",
    "            return response_string.split('ssistant\\n')[1].split('User')[0]\n",
    "    \n",
    "    def llm_create_system_prompt(self,prompt):  ### for instruct models\n",
    "            \n",
    "            return [\n",
    "                dict({\"role\": \"system\", \"content\": self.system_message}),\n",
    "                dict({\"role\": \"user\", \"content\": prompt}),\n",
    "            ]\n",
    "    \n",
    "        \n",
    "    def get_tool_schema(self,func: Callable) -> dict:\n",
    "        \"\"\"\n",
    "        Generate a JSON schema for a tool function.\n",
    "\n",
    "        Args:\n",
    "            func (Callable): The function to generate a schema for.\n",
    "\n",
    "        Returns:\n",
    "            dict: A JSON schema representing the function's parameters.\n",
    "        \"\"\"\n",
    "        import inspect\n",
    "        signature = inspect.signature(func)\n",
    "        parameters = {}\n",
    "\n",
    "        for name, param in signature.parameters.items():\n",
    "            parameters[name] = {\n",
    "                \"type\": \"string\",  # Assume string type for simplicity\n",
    "                \"description\": f\"Parameter {name}\"\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": func.__name__,\n",
    "                \"description\": func.__doc__.split('\\n')[0] if func.__doc__ else \"\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": parameters\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def llm_generate_response(self,prompt):  #### given we are using instruct model and it is  tools compatible (if tools are stated)\n",
    "        \n",
    "        # Generate response\n",
    "        messages =  self.llm_create_system_prompt(prompt)\n",
    "        schema_tools = []\n",
    "        \n",
    "        for tool in self.tools:\n",
    "            schema_tools.append(self.get_tool_schema(tool))\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tools= schema_tools,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Generate response\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", return_attention_mask=True).to(self.model.device)\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=384,\n",
    "            #pad_token_id=self.model.config.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode response\n",
    "        response = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        return response\n",
    "    \n",
    "    def generate_response(self, messages): # if the model is not instruct\n",
    "        # Prepare input\n",
    "        \n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Generate response\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", return_attention_mask=True).to(self.model.device)\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=384,\n",
    "            #pad_token_id=self.model.config.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode response\n",
    "        response = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        return response\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
