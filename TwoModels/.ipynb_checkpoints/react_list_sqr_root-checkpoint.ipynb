{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1651da74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install langgraph\n",
    "#!pip3  install torch torchvision torchaudio transformers\n",
    "#!pip3 install packaging ninja\n",
    "#!pip3 install accelerate\n",
    "#!pip3 install protobuf\n",
    "#!pip3 install sentencepiece\n",
    "#!pip3 install bitsandbytes\n",
    "#!pip3 install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1f71f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f6f9cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import json\n",
    "from typing import Dict, Any, Optional\n",
    "import torch\n",
    "class FunctionToolkit:\n",
    "    @staticmethod\n",
    "    def create_function_schema(func):\n",
    "        \"\"\"\n",
    "        Automatically generate a JSON schema for a given function\n",
    "        \"\"\"\n",
    "        # Extract function signature details\n",
    "        signature = inspect.signature(func)\n",
    "        \n",
    "        # Create JSON schema\n",
    "        schema = {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": func.__name__,\n",
    "                \"description\": func.__doc__ or \"No description provided\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {},\n",
    "                    \"required\": []\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Process parameters\n",
    "        for name, param in signature.parameters.items():\n",
    "            # Determine type\n",
    "            if param.annotation == int:\n",
    "                param_type = \"integer\"\n",
    "            elif param.annotation == float:\n",
    "                param_type = \"number\"\n",
    "            elif param.annotation == str:\n",
    "                param_type = \"string\"\n",
    "            else:\n",
    "                param_type = \"any\"\n",
    "            \n",
    "            # Add to properties\n",
    "            schema[\"function\"][\"parameters\"][\"properties\"][name] = {\n",
    "                \"type\": param_type\n",
    "            }\n",
    "            \n",
    "            # Check if parameter is required\n",
    "            if param.default == param.empty:\n",
    "                schema[\"function\"][\"parameters\"][\"required\"].append(name)\n",
    "        \n",
    "        return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53dad680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, LlamaTokenizer, LlamaForCausalLM, MistralForCausalLM\n",
    "import random, json\n",
    "import inspect\n",
    "import json\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, model,name):\n",
    "        # Load Qwen model and tokenizer            \n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\",\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,  # Changed from bfloat16 to float16\n",
    "            bnb_4bit_quant_storage=torch.uint8,    # Added for storage optimization\n",
    "            use_nested_quant=True,                 # Added for nested quantization\n",
    "        )\n",
    "        save_directory = model.replace('/','_')+'_saved'\n",
    "        try:\n",
    "            print('Trying to load the mode:',save_directory,'from local repo')\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "        except:  \n",
    "            print('The model:',model,'is not found locally, downloading it')\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model, quantization_config=bnb_config, use_auth_token=\"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\"\n",
    "            )\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=\"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\")\n",
    "            print(\"Saving the model:\",model,\" locally\")\n",
    "            self.model.save_pretrained(save_directory)\n",
    "            self.tokenizer.save_pretrained(save_directory)\n",
    "        self.name = name\n",
    "        self.model_name = model\n",
    "        self.tools = {\n",
    "            \"sqr_root\": self.sqr_root,\n",
    "            \"list_files\": self.list_files_in_directory\n",
    "        }\n",
    "        self.tool_schemas = {\n",
    "            name: FunctionToolkit.create_function_schema(func) \n",
    "            for name, func in self.tools.items()\n",
    "        }\n",
    "    def clear_response(self,messages, response_string):\n",
    "        #print('agent_name',agent_name)\n",
    "        if all(keyword in self.model_name for keyword in ['Qwen','Instruct']):\n",
    "            #print('//////////////',response_string,'\\n','//////////')\n",
    "            return response_string.replace('ssistant.','%').split('ssistant\\n')[1]\n",
    "        if all(keyword in self.model_name for keyword in ['falcon','instruct']): \n",
    "            return response_string.split('ssistant:')[1].split('User')[0]\n",
    "        if all(keyword in self.model_name for keyword in ['lama','nstruct']):\n",
    "            return response_string.split('ssistant\\n')[1].split('User')[0]\n",
    "        if all(keyword in self.model_name for keyword in ['mistralai','nstruct']):\n",
    "            return response_string[len(messages[0]['content'])+len(messages[1]['content'])+2:]\n",
    "        if all(keyword in self.model_name for keyword in ['OpenHermes','OpenHermes']):\n",
    "            return response_string.split('ssistant\\n')[1].split('User')[0]\n",
    "    \n",
    "    def list_files_in_directory(self, directory_path: str) -> list:\n",
    "        import os\n",
    "        files = os.listdir(directory_path)\n",
    "        return files\n",
    "    \n",
    "    def sqr_root(self, number: float)-> float:\n",
    "        \n",
    "        return float(number ** 0.5)\n",
    "    def create_system_prompt(self):\n",
    "        \"\"\"\n",
    "        Generate a system prompt that describes available tools\n",
    "        \"\"\"\n",
    "        # Convert tool schemas to a readable format\n",
    "        tools_description = json.dumps(\n",
    "            self.tool_schemas, \n",
    "            indent=2\n",
    "        )\n",
    "        \n",
    "        system_prompt = f\"\"\"\n",
    "AVAILABLE TOOLS:\n",
    "{tools_description}\n",
    "\n",
    "TOOL USAGE INSTRUCTIONS:\n",
    "1. You MUST ONLY use the available tools when EXPLICITLY asked about the same naming\n",
    "2. If the query is about 'square' only without 'root', do NOT use any tool\n",
    "3. For square root calculations, use the tool ONLY when:\n",
    "   - Phrase includes 'square root'\n",
    "   - Request is to find âˆš of a number\n",
    "4. If unsure, respond without using any tool\n",
    "5. Be precise and follow these instructions strictly\n",
    "6.You have access to a tool that can list files in a specified directory. Please use this tool to list the files whenever you are asked to.\n",
    "\n",
    "Example Tool Call Format:\n",
    "```\n",
    "TOOL_CALL: {{\n",
    "    \"name\": \"tool_name\",\n",
    "    \"arguments\": {{\n",
    "        \"argument1\": value1,\n",
    "        \"argument2\": value2\n",
    "    }}\n",
    "}}\n",
    "\"\"\"\n",
    "        return system_prompt\n",
    "    def generate_response_with_Action(self, user_prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response with potential tool usage\n",
    "        \n",
    "        Args:\n",
    "            user_prompt (str): The user's input query\n",
    "        \n",
    "        Returns:\n",
    "            str: Model's generated response\n",
    "        \"\"\"\n",
    "        # Combine system prompt with user prompt\n",
    "        full_prompt = self.create_system_prompt() + \"\\n\\nUSER QUERY: \" + user_prompt\n",
    "\n",
    "        # Tokenize the input\n",
    "        inputs = self.tokenizer(full_prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        # Generate response\n",
    "        outputs = self.model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        # Decode the response\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        #response = response[len(self.create_system_prompt()):]\n",
    "        \n",
    "        return response\n",
    "\n",
    "    def execute_tool_call(self, response: str) -> Optional[Any]:\n",
    "        \"\"\"\n",
    "        Parse and execute tool calls from the model's response\n",
    "        \n",
    "        Args:\n",
    "            response (str): The model's generated response\n",
    "        \n",
    "        Returns:\n",
    "            Optional result of the tool call\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Regex to extract tool call details\n",
    "        tool_match = re.search(r'TOOL_CALL:\\s*({[^}]+})', response)\n",
    "        \n",
    "        if tool_match:\n",
    "            try:\n",
    "                # Parse the tool call details\n",
    "                tool_details = json.loads(tool_match.group(1))\n",
    "                \n",
    "                # Check if the tool exists\n",
    "                if tool_details['name'] in self.tools:\n",
    "                    # Call the tool with its arguments\n",
    "                    tool = self.tools[tool_details['name']]\n",
    "                    result = tool(**tool_details['arguments'])\n",
    "                    \n",
    "                    return result\n",
    "            except Exception as e:\n",
    "                return f\"Error executing tool: {e}\"\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def generate_response(self, messages):\n",
    "        # Prepare input\n",
    "        tools = [self.sqr_root,  self.list_files_in_directory]\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Generate response\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", return_attention_mask=True).to(self.model.device)\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=200,\n",
    "            #pad_token_id=self.model.config.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode response\n",
    "        response = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c28284f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_setter = None\n",
    "guesser = None\n",
    "react_agent = None\n",
    "cleaner_agent = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb5e5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89646582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global react_agent, cleaner_agent\n",
    "    READER_MODEL_NAME1 = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "    READER_MODEL_NAME2 = \"tiiuae/falcon-7b-instruct\"\n",
    "    READER_MODEL_NAME3 = 'teknium/OpenHermes-2.5-Mistral-7B'\n",
    "    READER_MODEL_NAME4 = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "    READER_MODEL_NAME5 = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    if react_agent is None:\n",
    "        react_agent = Agent(READER_MODEL_NAME5,\"react\")\n",
    "        cleaner_agent = Agent(READER_MODEL_NAME1,'cleaner')\n",
    "    thedir = react_agent.list_files_in_directory(\"/\")\n",
    "    \n",
    "    question = 'What is the square root of 30  ?'\n",
    "    question = 'list all my files in / directory'\n",
    "    messages = [{\"role\":\"system\",\"content\":react_agent.create_system_prompt()},\n",
    "                {\"role\":\"user\",\"content\":question}]\n",
    "    #agent_response = react_agent.generate_response(messages)\n",
    "    #agent_response = react_agent.clear_response(messages, agent_response)\n",
    "    agent_response = react_agent.generate_response_with_Action(question)\n",
    "    #concise = react_agent.execute_tool_call(agent_response)\n",
    "    #if concise is not None:\n",
    "    #    print(concise)\n",
    "    #else:\n",
    "\n",
    "    cleaner_prompt=[{\"role\":\"system\",\"content\":\"You are smart text understanding expert, Extract the results from the given prompt\\\n",
    "     Ignore the texts that has not meaning and just extract the results\"},\n",
    "                   {\"role\":\"user\",\"content\":agent_response}]\n",
    "    filter_prompt = f\"\"\"\n",
    "You are a precise extraction assistant. Your ONLY task is to find and return the EXACT numerical result from the given text. \n",
    "\n",
    "Rules:\n",
    "- Look for the single, precise explanation output\n",
    "- if it is numerical  reply only with the numerical\n",
    "- Ignore all surrounding text or context\n",
    "- If no clear numerical result is found, respond with the found explanation\n",
    "- Extract all the explanation if it shows that there is no results or otherwise, extract ONLY the numerical value\n",
    "\n",
    "\"\"\"\n",
    "    cleaner_prompt = [{\"role\":\"system\",\"content\":filter_prompt}, {\"role\":\"user\",\"content\":agent_response}]\n",
    "    cleaner_response = cleaner_agent.generate_response(cleaner_prompt)\n",
    "    cleaned_response = cleaner_agent.clear_response(cleaner_prompt, cleaner_response)\n",
    "    try:\n",
    "        print(';;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;')\n",
    "        print('agent_resposne:',agent_response)\n",
    "        print(';;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;')\n",
    "        print('cleaner_response:',cleaner_response)\n",
    "        print(';;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;')\n",
    "        print('cleaned_response:',cleaned_response)\n",
    "    except:\n",
    "        try:\n",
    "            print(agent_response.split(\"USER RESPONSE:\")[1])\n",
    "        except:\n",
    "            print(agent_response[len(react_agent.create_system_prompt()):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b86342bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['torch_dtype', 'device_map', 'use_nested_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load the mode: mistralai_Mistral-7B-Instruct-v0.3_saved from local repo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Unused kwargs: ['torch_dtype', 'device_map', 'use_nested_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load the mode: Qwen_Qwen2.5-Coder-7B-Instruct_saved from local repo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a01484d7844d498a83fcc3b48bb4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
      "agent_resposne: \n",
      "AVAILABLE TOOLS:\n",
      "{\n",
      "  \"sqr_root\": {\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "      \"name\": \"sqr_root\",\n",
      "      \"description\": \"No description provided\",\n",
      "      \"parameters\": {\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {\n",
      "          \"number\": {\n",
      "            \"type\": \"number\"\n",
      "          }\n",
      "        },\n",
      "        \"required\": [\n",
      "          \"number\"\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"list_files\": {\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "      \"name\": \"list_files_in_directory\",\n",
      "      \"description\": \"No description provided\",\n",
      "      \"parameters\": {\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {\n",
      "          \"directory_path\": {\n",
      "            \"type\": \"string\"\n",
      "          }\n",
      "        },\n",
      "        \"required\": [\n",
      "          \"directory_path\"\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "TOOL USAGE INSTRUCTIONS:\n",
      "1. You MUST ONLY use the available tools when EXPLICITLY asked about the same naming\n",
      "2. If the query is about 'square' only without 'root', do NOT use any tool\n",
      "3. For square root calculations, use the tool ONLY when:\n",
      "   - Phrase includes 'square root'\n",
      "   - Request is to find âˆš of a number\n",
      "4. If unsure, respond without using any tool\n",
      "5. Be precise and follow these instructions strictly\n",
      "6.You have access to a tool that can list files in a specified directory. Please use this tool to list the files whenever you are asked to.\n",
      "\n",
      "Example Tool Call Format:\n",
      "```\n",
      "TOOL_CALL: {\n",
      "    \"name\": \"tool_name\",\n",
      "    \"arguments\": {\n",
      "        \"argument1\": value1,\n",
      "        \"argument2\": value2\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "USER QUERY: list all my files in / directory\n",
      "\n",
      "RESPONSE:\n",
      "TOOL_CALL: {\n",
      "    \"name\": \"list_files\",\n",
      "    \"arguments\": {\n",
      "        \"directory_path\": \"/\"\n",
      "    }\n",
      "}\n",
      "```\n",
      ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
      "cleaner_response: system\n",
      "\n",
      "You are a precise extraction assistant. Your ONLY task is to find and return the EXACT numerical result from the given text. \n",
      "\n",
      "Rules:\n",
      "- Look for the single, precise explanation output\n",
      "- if it is numerical  reply only with the numerical\n",
      "- Ignore all surrounding text or context\n",
      "- If no clear numerical result is found, respond with the found explanation\n",
      "- Extract all the explanation if it shows that there is no results or otherwise, extract ONLY the numerical value\n",
      "\n",
      "\n",
      "user\n",
      "\n",
      "AVAILABLE TOOLS:\n",
      "{\n",
      "  \"sqr_root\": {\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "      \"name\": \"sqr_root\",\n",
      "      \"description\": \"No description provided\",\n",
      "      \"parameters\": {\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {\n",
      "          \"number\": {\n",
      "            \"type\": \"number\"\n",
      "          }\n",
      "        },\n",
      "        \"required\": [\n",
      "          \"number\"\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"list_files\": {\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "      \"name\": \"list_files_in_directory\",\n",
      "      \"description\": \"No description provided\",\n",
      "      \"parameters\": {\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {\n",
      "          \"directory_path\": {\n",
      "            \"type\": \"string\"\n",
      "          }\n",
      "        },\n",
      "        \"required\": [\n",
      "          \"directory_path\"\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "TOOL USAGE INSTRUCTIONS:\n",
      "1. You MUST ONLY use the available tools when EXPLICITLY asked about the same naming\n",
      "2. If the query is about 'square' only without 'root', do NOT use any tool\n",
      "3. For square root calculations, use the tool ONLY when:\n",
      "   - Phrase includes 'square root'\n",
      "   - Request is to find âˆš of a number\n",
      "4. If unsure, respond without using any tool\n",
      "5. Be precise and follow these instructions strictly\n",
      "6.You have access to a tool that can list files in a specified directory. Please use this tool to list the files whenever you are asked to.\n",
      "\n",
      "Example Tool Call Format:\n",
      "```\n",
      "TOOL_CALL: {\n",
      "    \"name\": \"tool_name\",\n",
      "    \"arguments\": {\n",
      "        \"argument1\": value1,\n",
      "        \"argument2\": value2\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "USER QUERY: list all my files in / directory\n",
      "\n",
      "RESPONSE:\n",
      "TOOL_CALL: {\n",
      "    \"name\": \"list_files\",\n",
      "    \"arguments\": {\n",
      "        \"directory_path\": \"/\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "assistant\n",
      "TOOL_CALL: {\n",
      "    \"name\": \"list_files\",\n",
      "    \"arguments\": {\n",
      "        \"directory_path\": \"/\"\n",
      "    }\n",
      "}\n",
      ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
      "cleaned_response: TOOL_CALL: {\n",
      "    \"name\": \"list_files\",\n",
      "    \"arguments\": {\n",
      "        \"directory_path\": \"/\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9522773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3677606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
