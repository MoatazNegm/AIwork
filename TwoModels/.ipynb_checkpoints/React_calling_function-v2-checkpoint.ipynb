{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1651da74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install langgraph\n",
    "#!pip3  install torch torchvision torchaudio transformers\n",
    "#!pip3 install packaging ninja\n",
    "#!pip3 install accelerate\n",
    "#!pip3 install protobuf\n",
    "#!pip3 install sentencepiece\n",
    "#!pip3 install bitsandbytes\n",
    "#!pip3 install scipy\n",
    "tool_call_str = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1f71f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f6f9cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import json\n",
    "from typing import Dict, Any, Optional\n",
    "import torch\n",
    "class FunctionToolkit:\n",
    "    @staticmethod\n",
    "    def create_function_schema(func):\n",
    "        \"\"\"\n",
    "        Automatically generate a JSON schema for a given function\n",
    "        \"\"\"\n",
    "        # Extract function signature details\n",
    "        signature = inspect.signature(func)\n",
    "        \n",
    "        # Create JSON schema\n",
    "        schema = {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": func.__name__,\n",
    "                \"description\": func.__doc__ or \"No description provided\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {},\n",
    "                    \"required\": []\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Process parameters\n",
    "        for name, param in signature.parameters.items():\n",
    "            # Determine type\n",
    "            if param.annotation == int:\n",
    "                param_type = \"integer\"\n",
    "            elif param.annotation == float:\n",
    "                param_type = \"number\"\n",
    "            elif param.annotation == str:\n",
    "                param_type = \"string\"\n",
    "            else:\n",
    "                param_type = \"any\"\n",
    "            \n",
    "            # Add to properties\n",
    "            schema[\"function\"][\"parameters\"][\"properties\"][name] = {\n",
    "                \"type\": param_type\n",
    "            }\n",
    "            \n",
    "            # Check if parameter is required\n",
    "            if param.default == param.empty:\n",
    "                schema[\"function\"][\"parameters\"][\"required\"].append(name)\n",
    "        \n",
    "        return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53dad680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch, os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, LlamaTokenizer, LlamaForCausalLM, MistralForCausalLM\n",
    "import random, json\n",
    "import inspect\n",
    "import json\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, model,name):\n",
    "        # Load Qwen model and tokenizer            \n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\",\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quantw_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,  # Changed from bfloat16 to float16\n",
    "            bnb_4bit_quant_storage=torch.uint8,    # Added for storage optimization\n",
    "            use_nested_quant=True,                 # Added for nested quantization\n",
    "        )\n",
    "        save_directory = model.replace('/','_')+'_saved'\n",
    "        try:\n",
    "            print('Trying to load the mode:',save_directory,'from local repo')\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "        except:  \n",
    "            print('The model:',model,'is not found locally, downloading it')\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model, quantization_config=bnb_config, use_auth_token=\"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\"\n",
    "            )\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=\"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\")\n",
    "            print(\"Saving the model:\",model,\" locally\")\n",
    "            self.model.save_pretrained(save_directory)\n",
    "            self.tokenizer.save_pretrained(save_directory)\n",
    "        self.name = name\n",
    "        self.model_name = model\n",
    "        self.tools = {\n",
    "            \"get_status\": self.getstatus,\n",
    "             \"list_files\": self.list_files_in_directory\n",
    "        }\n",
    "        #self.tool_schemas = {\n",
    "        #    name: FunctionToolkit.create_function_schema(func) \n",
    "        #    for name, func in self.tools.items()\n",
    "        #}\n",
    "    def clear_response(self,messages, response_string):\n",
    "        #print('agent_name',agent_name)\n",
    "        if all(keyword in self.model_name for keyword in ['Qwen','Instruct']):\n",
    "            #print('//////////////',response_string,'\\n','//////////')\n",
    "            return response_string.replace('ssistant.','%').split('ssistant\\n')[1]\n",
    "        if all(keyword in self.model_name for keyword in ['falcon','instruct']): \n",
    "            return response_string.split('ssistant:')[1].split('User')[0]\n",
    "        if all(keyword in self.model_name for keyword in ['lama','nstruct']):\n",
    "            return response_string.split('ssistant\\n')[1].split('User')[0]\n",
    "        if all(keyword in self.model_name for keyword in ['mistralai','nstruct']):\n",
    "            return response_string[len(messages[0]['content'])+len(messages[1]['content'])+2:]\n",
    "        if all(keyword in self.model_name for keyword in ['OpenHermes','OpenHermes']):\n",
    "            return response_string.split('ssistant\\n')[1].split('User')[0]\n",
    "    \n",
    "    def getstatus(self,opt:str='1') -> str:\n",
    "       \n",
    "        #files = os.listdir('/')\n",
    "        print('hello from moataz')\n",
    "        return 'hello world'\n",
    "    \n",
    "    def list_files_in_directory(self, number: float)-> float:\n",
    "        \n",
    "        return str(float(number ** 0.5))\n",
    "    \n",
    "    def create_system_prompt(self):\n",
    "        \"\"\"\n",
    "        Generate a system prompt that describes available tools\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert tool schemas to a readable format\n",
    "        #tools_description = json.dumps(\n",
    "        #    self.tool_schemas, \n",
    "        #    indent=2\n",
    "        #)\n",
    "        \n",
    "        system_prompt = {\"role\":\"tool_calls\",\"content\":'tools_description'}\n",
    "        system_prompt = {\n",
    "        \"role\": \"tool_calls\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"name\": \"getstatus\",\n",
    "                \"description\": \"Get▁the▁current▁status\",\n",
    "                \"parameters\": {\"type\": \"object\", \"properties\": {\"opt\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"}\n",
    "               \n",
    "                \n",
    "                                                                }\n",
    "                              }\n",
    "            }\n",
    "        ]\n",
    "    }  \n",
    "\n",
    "        return system_prompt\n",
    "   \n",
    "    def generate_response_with_Action(self, user_prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a response with potential tool usage\n",
    "\n",
    "        Args:\n",
    "            user_prompt (str): The user's input query\n",
    "\n",
    "        Returns:\n",
    "            str: Model's generated response\n",
    "        \"\"\"\n",
    "        # Check if the user prompt suggests using a tool\n",
    "        if 'get status' in user_prompt.lower():\n",
    "            try:\n",
    "                # Extract the number from the prompt\n",
    "                \n",
    "                # Directly call the square root tool\n",
    "                result = self.getstatus()\n",
    "\n",
    "                # Prepare a response that includes the tool resul\n",
    "                #return f\"The status  is  {result}, TOOL_CALL: \"+\"\\{\\\"name\\\":\\\"get_status\\\",\\\"arguments\\\": \\{\\}\\}\"\n",
    "            except (ValueError, IndexError):\n",
    "                print('eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeerrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr')\n",
    "                pass\n",
    "\n",
    "        # Combine system prompt with user prompt\n",
    "        full_prompt = self.create_system_prompt() + \"\\n\\nUSER QUERY: \" + user_prompt\n",
    "        \n",
    "        tools =  [{\"type\": \"function\", \"function\":  \n",
    "                           {\"name\":\"getstatus\", \"description\": \"Get▁the▁current▁status\",\n",
    "                            \"parameters\": {\"type\": \"object\", \"properties\": {\"opt\": \n",
    "                                    {\"type\": \"string\", \"description\": \"The city and status\"}},\"required\":[]}}}]\n",
    "\n",
    "        # Tokenize the input\n",
    "        inputs = self.tokenizer(full_prompt, return_tensors=\"pt\", tools = tools).to(self.model.device)\n",
    "\n",
    "        # Generate response\n",
    "        outputs = self.model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "            temperature=0.2\n",
    "        )\n",
    "\n",
    "        # Decode the response\n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print('ppppppppppppppppppppppppppppppp')\n",
    "        print(response)\n",
    "        exit()\n",
    "        # Try to execute any tool call in the response\n",
    "        tool_result = self.execute_tool_call(response)\n",
    "        if tool_result:\n",
    "            return f\"{response}\\n\\nTool Result: {tool_result}\"\n",
    "\n",
    "        return response\n",
    "\n",
    "    def execute_tool_call(self, response: str) -> Optional[Any]:\n",
    "        global tool_call_str\n",
    "        import re\n",
    "        import ast\n",
    "\n",
    "        # More flexible regex to find tool calls\n",
    "        tool_match = re.search(r'TOOL_CALL:\\s*(\\{[^}]+\\})', response, re.DOTALL)\n",
    "        print('hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh')\n",
    "        if tool_match:\n",
    "            if tool_details.get('name') in self.tools:\n",
    "                # Get the tool function\n",
    "                print( 'sssssssssssssssssssssssssssssssssss3')\n",
    "                tool = self.tools[tool_details['name']]\n",
    "                print('sssssstool:',tool)\n",
    "                # Call the tool with no arguments or empty dict\n",
    "                if callable(tool):\n",
    "                    result = tool()\n",
    "                    print( 'sssssssssssssssssssssssssssssssssss33')\n",
    "                else:\n",
    "                    result = tool\n",
    "                    print( 'sssssssssssssssssssssssssssssssssss333')\n",
    "\n",
    "                return result\n",
    "            try:\n",
    "                print( 'sssssssssssssssssssssssssssssssssss2')\n",
    "                # Try to parse the tool call JSON\n",
    "                tool_details = json.loads(tool_match.group(1))\n",
    "                \n",
    "                # Check if the tool exists\n",
    "                if tool_details.get('name') in self.tools:\n",
    "                    # Get the tool function\n",
    "                    print( 'sssssssssssssssssssssssssssssssssss3')\n",
    "                    tool = self.tools[tool_details['name']]\n",
    "\n",
    "                    # Call the tool with no arguments or empty dict\n",
    "                    if callable(tool):\n",
    "                        result = tool()\n",
    "                        print( 'sssssssssssssssssssssssssssssssssss33')\n",
    "                    else:\n",
    "                        result = tool\n",
    "                        print( 'sssssssssssssssssssssssssssssssssss333')\n",
    "\n",
    "                    return result\n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print( 'sssssssssssssssssssssssssssssssssss4')\n",
    "                return f\"JSON Parsing Error: {e}\"\n",
    "            except Exception as e:\n",
    "                print( 'sssssssssssssssssssssssssssssssssss5')\n",
    "                return f\"Error executing tool: {e}\"\n",
    "\n",
    "        return None\n",
    "    def llama_response(self,text):\n",
    "        \n",
    "        # Generate response\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", return_attention_mask=True).to(self.model.device)\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=200,\n",
    "            #pad_token_id=self.model.config.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode response\n",
    "        response = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        return response\n",
    "    def generate_response(self, messages):\n",
    "        # Prepare input\n",
    "        tools = [self.getstatus]\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Generate response\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", return_attention_mask=True).to(self.model.device)\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=200,\n",
    "            #pad_token_id=self.model.config.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode response\n",
    "        response = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c28284f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_setter = None\n",
    "guesser = None\n",
    "react_agent = None\n",
    "cleaner_agent = None\n",
    "llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb5e5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89646582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global react_agent, cleaner_agent\n",
    "    READER_MODEL_NAME1 = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "    READER_MODEL_NAME2 = \"tiiuae/falcon-7b-instruct\"\n",
    "    READER_MODEL_NAME3 = 'teknium/OpenHermes-2.5-Mistral-7B'\n",
    "    READER_MODEL_NAME4 = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "    READER_MODEL_NAME5 = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    if react_agent is None:\n",
    "        react_agent = Agent(READER_MODEL_NAME5,\"react\")\n",
    "        cleaner_agent = Agent(READER_MODEL_NAME1,'cleaner')\n",
    "    #thedir = react_agent.list_files_in_directory(\"/\")\n",
    "    \n",
    "    question = 'get status ?'\n",
    "    #question = 'list all my files in / directory'\n",
    "    messages = [react_agent.create_system_prompt(),\n",
    "                {\"role\":\"user\",\"content\":question}]\n",
    "    print('messeges',type(messages))\n",
    "    print(react_agent.create_system_prompt())\n",
    "   \n",
    "    agent_response = react_agent.generate_response(messages)\n",
    "    \n",
    "    return\n",
    "    #agent_response = react_agent.clear_response(messages, agent_response)\n",
    "    #agent_response = react_agent.generate_response_with_Action(question)\n",
    "    #concise = react_agent.execute_tool_call(agent_response)\n",
    "    #if concise is not None:\n",
    "    #    print(concise)\n",
    "    #else:\n",
    "\n",
    "    cleaner_prompt=[{\"role\":\"system\",\"content\":\"You are smart text understanding expert, Extract the results from the given prompt\\\n",
    "     Ignore the texts that has not meaning and just extract the results\"},\n",
    "                   {\"role\":\"user\",\"content\":agent_response}]\n",
    "    filter_prompt = f\"\"\"\n",
    "You are a precise extraction assistant. Your ONLY task is to find and return the EXACT numerical result from the given text. \n",
    "\n",
    "Rules:\n",
    "- Look for the single, precise explanation output\n",
    "- if it is numerical  reply only with the numerical\n",
    "- Ignore all surrounding text or context\n",
    "- If no clear numerical result is found, respond with the found explanation\n",
    "- Extract all the explanation if it shows that there is no results or otherwise, extract ONLY the numerical value\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    cleaner_prompt = [{\"role\":\"system\",\"content\":filter_prompt}, {\"role\":\"user\",\"content\":agent_response}]\n",
    "    cleaner_response = cleaner_agent.generate_response(cleaner_prompt)\n",
    "    cleaned_response = cleaner_agent.clear_response(cleaner_prompt, cleaner_response)\n",
    "    try:\n",
    "        print(';;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;')\n",
    "        print('agent_resposne:',agent_response)\n",
    "        print(';;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;')\n",
    "        print('cleaner_response:',cleaner_response)\n",
    "        print(';;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;')\n",
    "        print('cleaned_response:',cleaned_response)\n",
    "    except:\n",
    "        try:\n",
    "            print(agent_response.split(\"USER RESPONSE:\")[1])\n",
    "        except:\n",
    "            print(agent_response[len(react_agent.create_system_prompt()):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b86342bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['torch_dtype', 'device_map', 'bnb_4bit_quantw_type', 'use_nested_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load the mode: mistralai_Mistral-7B-Instruct-v0.3_saved from local repo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['torch_dtype', 'device_map', 'bnb_4bit_quantw_type', 'use_nested_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load the mode: Qwen_Qwen2.5-Coder-7B-Instruct_saved from local repo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a370098bea5459eaed0d07b5f1c0ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messeges <class 'list'>\n",
      "{'role': 'tool_calls', 'content': [{'type': 'function', 'name': 'getstatus', 'description': 'Get▁the▁current▁status', 'parameters': {'type': 'object', 'properties': {'opt': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}}}}]}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tools should either be a JSON schema, or a callable function with type hints and a docstring suitable for auto-conversion to a schema.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmesseges\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mtype\u001b[39m(messages))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(react_agent\u001b[38;5;241m.\u001b[39mcreate_system_prompt())\n\u001b[0;32m---> 20\u001b[0m agent_response \u001b[38;5;241m=\u001b[39m \u001b[43mreact_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#agent_response = react_agent.clear_response(messages, agent_response)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#agent_response = react_agent.generate_response_with_Action(question)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#concise = react_agent.execute_tool_call(agent_response)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#if concise is not None:\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#    print(concise)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#else:\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 212\u001b[0m, in \u001b[0;36mAgent.generate_response\u001b[0;34m(self, messages)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, messages):\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# Prepare input\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     tools \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetstatus]\n\u001b[0;32m--> 212\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1814\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1812\u001b[0m             tool_schemas\u001b[38;5;241m.\u001b[39mappend(get_json_schema(tool))\n\u001b[1;32m   1813\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1815\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTools should either be a JSON schema, or a callable function with type hints \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1816\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand a docstring suitable for auto-conversion to a schema.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1817\u001b[0m             )\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1819\u001b[0m     tool_schemas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Tools should either be a JSON schema, or a callable function with type hints and a docstring suitable for auto-conversion to a schema."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9522773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3287ba9a8a49898691895aa75e3a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29fb45c76cab456ba64136749acbc9fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model.v3:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3b37ad32924395b8c51d2d46add0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.json:   0%|          | 0.00/202 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0904041242814411ab12af3626425d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "consolidated.safetensors:   0%|          | 0.00/14.5G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/workspace/moataz-work/TwoModels/mistral_models/7B-Instruct-v0.3'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3677606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb8d18cd7c354463ad25abbe7a96e24c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654a73345b7d4e1d9bc483c86f105820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.json:   0%|          | 0.00/283 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd639e104d84c189651934b39902ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "consolidated.safetensors:   0%|          | 0.00/16.0G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb4e43421084b99a9c82cebd8319483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tekken.json:   0%|          | 0.00/14.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/workspace/moataz-work/TwoModels/mistral_models/8B-Instruct'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "mistral_models_path = Path('/workspace/moataz-work/TwoModels/').joinpath('mistral_models', '8B-Instruct')\n",
    "mistral_models_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "snapshot_download(repo_id=\"mistralai/Ministral-8B-Instruct-2410\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tekken.json\"], token='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', local_dir=mistral_models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "505025bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['torch_dtype', 'device_map', 'bnb_4bit_quantw_type', 'use_nested_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load the mode: meta-llama_Llama-3.1-8B_saved from local repo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f47756c9b34db799761158d9964c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You have access to a function `get_status()` which returns the current system status.\n",
      "When asked about the system's status, you should call this function.\n",
      "\n",
      "Example interaction:\n",
      "User: What is the current system status?\n",
      "Assistant: I'll check the system status for you.\n",
      "[Function Call: get_status()]\n",
      "Result: System is operational and running smoothly\n",
      "Response: The system is currently operational and running smoothly.\n",
      "\n",
      "\n",
      "User: What is the current system status? \n",
      "Assistant: I'll check the system status for you. \n",
      "[Function Call: get_status()] \n",
      "Result: System is operational and running smoothly \n",
      "Response: The system is currently operational and running smoothly. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load Llama 3 model\n",
    "if llm == None:\n",
    "    llm = Agent('meta-llama/Llama-3.1-8B','executor')\n",
    "# Define the function to be called\n",
    "def get_status():\n",
    "    return \"System is operational and running smoothly\"\n",
    "\n",
    "# Create a function calling prompt\n",
    "def create_function_calling_prompt():\n",
    "    return \"\"\"\n",
    "You have access to a function `get_status()` which returns the current system status.\n",
    "When asked about the system's status, you should call this function.\n",
    "\n",
    "Example interaction:\n",
    "User: What is the current system status?\n",
    "Assistant: I'll check the system status for you.\n",
    "[Function Call: get_status()]\n",
    "Result: System is operational and running smoothly\n",
    "Response: The system is currently operational and running smoothly.\n",
    "\"\"\"\n",
    "\n",
    "# Prepare the input with function definition and context\n",
    "input_text = create_function_calling_prompt() + \"\\n\\nUser: What is the current system status?\"\n",
    "response = llm.llama_response(input_text)\n",
    "# Encode the input\n",
    "#inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate response\n",
    "#outputs = model.generate(\n",
    "#    inputs.input_ids, \n",
    "#    max_new_tokens=100,\n",
    "#    do_sample=True,\n",
    "#    temperature=0.7\n",
    "#)\n",
    "\n",
    "# Decode the response\n",
    "#response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fb5028",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
