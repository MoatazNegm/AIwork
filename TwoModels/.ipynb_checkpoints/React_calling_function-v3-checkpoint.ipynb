{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1f71f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53dad680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch, os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, LlamaTokenizer, LlamaForCausalLM, MistralForCausalLM\n",
    "import random, json\n",
    "import inspect\n",
    "import json\n",
    "from typing import Dict, Any, Optional, Callable, List\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, model,name):\n",
    "        # Load Qwen model and tokenizer            \n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\",\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quantw_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,  # Changed from bfloat16 to float16\n",
    "            bnb_4bit_quant_storage=torch.uint8,    # Added for storage optimization\n",
    "            use_nested_quant=True,                 # Added for nested quantization\n",
    "        )\n",
    "        save_directory = model.replace('/','_')+'_saved_response'\n",
    "        try:\n",
    "            print('Trying to load the mode:',save_directory,'from local repo')\n",
    "\n",
    "            #self.model = pipeline.load_from_pretrained(save_directory)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "        except:  \n",
    "            print('The model:',model,'is not found locally, downloading it')\n",
    "            #self.model = pipeline(\n",
    "            #    \"text-generation\",\n",
    "            #    model=model,\n",
    "            #    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            #    device_map=\"auto\",\n",
    "            #    token=\"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\"\n",
    "            #)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model, quantization_config=bnb_config, token=\"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\"\n",
    "            )\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model, token=\"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\")\n",
    "            print(\"Saving the model:\",model,\" locally\")\n",
    "            #self.model.save_pretrained(save_directory)\n",
    "            self.model.save_pretrained(save_directory)\n",
    "            self.tokenizer.save_pretrained(save_directory)\n",
    "        self.name = name\n",
    "        self.model_name = model\n",
    "        self.system_message = \"\"\n",
    "        self.tools = []\n",
    "        \n",
    "    def clear_response(self,messages, response_string):\n",
    "        #print('agent_name',agent_name)\n",
    "        if all(keyword in self.model_name for keyword in ['Qwen','Instruct']):\n",
    "            #print('//////////////',response_string,'\\n','//////////')\n",
    "            return response_string.replace('ssistant.','%').split('ssistant\\n')[1]\n",
    "        if all(keyword in self.model_name for keyword in ['falcon','instruct']): \n",
    "            return response_string.split('ssistant:')[1].split('User')[0]\n",
    "        if all(keyword in self.model_name for keyword in ['lama','nstruct']):\n",
    "            return response_string.split('ssistant\\n')[1].split('User')[0]\n",
    "        if all(keyword in self.model_name for keyword in ['mistralai','nstruct']):\n",
    "            return response_string[len(messages[0]['content'])+len(messages[1]['content'])+2:]\n",
    "        if all(keyword in self.model_name for keyword in ['OpenHermes','OpenHermes']):\n",
    "            return response_string.split('ssistant\\n')[1].split('User')[0]\n",
    "    \n",
    "    \n",
    "    def sqr_root(self, number: float)-> float:\n",
    "        \n",
    "        return float(number ** 0.5)\n",
    "    def llm_create_system_prompt(self,prompt):\n",
    "            \n",
    "            return [\n",
    "                dict({\"role\": \"system\", \"content\": self.system_message}),\n",
    "                dict({\"role\": \"user\", \"content\": prompt}),\n",
    "            ]\n",
    "        \n",
    "            functions = 'get_status(), self.list_files(directory)'\n",
    "            '''\n",
    "            return f\"\"\"\n",
    "You have access to a a list of functions {functions} which returns the current system status.\n",
    "When asked about the system's status, you should call this function.\n",
    "\n",
    "Example interaction:\n",
    "User: What is the current system status?\n",
    "Assistant:\n",
    "[Function Call: get_status()]\n",
    "Result: System is operational and running smoothly\n",
    "Response: The system is currently operational and running smoothly\n",
    "Another Example:\n",
    "User: what is listed in the directory: /home`\n",
    "Assistant:\n",
    "[Function Call: self.list_files(/home)]\n",
    "Result: bla1, bla2,...\n",
    "\"\"\"\n",
    "        '''\n",
    "        \n",
    "    def create_system_prompt(self):\n",
    "        \"\"\"\n",
    "        Generate a system prompt that describes available tools\n",
    "        \"\"\"\n",
    "        system_prompt = {\"role\":\"tool_calls\",\"content\":'tools_description'}\n",
    "        system_prompt = {\n",
    "        \"role\": \"tool_calls\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"name\": \"get_status\",\n",
    "                \"description\": \"Get▁the▁current▁status\",\n",
    "                \"parameters\": {\"type\": \"object\", \"properties\": {\"opt\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"}\n",
    "               \n",
    "                \n",
    "                                                                }\n",
    "                              }\n",
    "            }\n",
    "        ]\n",
    "    }  \n",
    "\n",
    "        return system_prompt\n",
    "    \n",
    "        \n",
    "    def get_tool_schema(self,func: Callable) -> dict:\n",
    "        \"\"\"\n",
    "        Generate a JSON schema for a tool function.\n",
    "\n",
    "        Args:\n",
    "            func (Callable): The function to generate a schema for.\n",
    "\n",
    "        Returns:\n",
    "            dict: A JSON schema representing the function's parameters.\n",
    "        \"\"\"\n",
    "        import inspect\n",
    "        signature = inspect.signature(func)\n",
    "        parameters = {}\n",
    "\n",
    "        for name, param in signature.parameters.items():\n",
    "            parameters[name] = {\n",
    "                \"type\": \"string\",  # Assume string type for simplicity\n",
    "                \"description\": f\"Parameter {name}\"\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"description\": func.__doc__.split('\\n')[0] if func.__doc__ else \"\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": parameters\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def llm_generate_response(self,prompt):\n",
    "        \n",
    "        # Generate response\n",
    "        messages =  self.llm_create_system_prompt(prompt)\n",
    "        schema_tools = []\n",
    "        \n",
    "        for tool in self.tools:\n",
    "            schema_tools.append(self.get_tool_schema(tool))\n",
    "            \n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tools= self.tools,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Generate response\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", return_attention_mask=True).to(self.model.device)\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=384,\n",
    "            #pad_token_id=self.model.config.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode response\n",
    "        response = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        # Decode response\n",
    "        \n",
    "        #outputs = pipeline(\n",
    "        #    messages,\n",
    "        #    max_new_tokens=256,\n",
    "        #)\n",
    "        return response\n",
    "    \n",
    "    def generate_response(self, messages):\n",
    "        # Prepare input\n",
    "        \n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Generate response\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", return_attention_mask=True).to(self.model.device)\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=384,\n",
    "            #pad_token_id=self.model.config.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decode response\n",
    "        response = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c28284f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_setter = None\n",
    "guesser = None\n",
    "react_agent = None\n",
    "cleaner_agent = None\n",
    "llm = None\n",
    "rephrase_agent = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c56279a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(directory:str) -> str:\n",
    "    \"\"\"\n",
    "        List files in the specified directory.\n",
    "\n",
    "        Args:\n",
    "            directory (str): The path to the directory to list files from.\n",
    "\n",
    "        Returns:\n",
    "            str: A comma-separated string of files in the directory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        files = os.listdir(directory)\n",
    "        return ', '.join(files)\n",
    "    except Exception as e:\n",
    "        return f\"Error listing files: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bb5e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_status()->str:\n",
    "    \"\"\"\n",
    "    Get the current status\n",
    "    \n",
    "    Args:\n",
    "        {}\n",
    "        \n",
    "    Returns:\n",
    "        The current status as a string.\n",
    "    \"\"\"\n",
    "    \n",
    "    return \"my status is great\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de280b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match <re.Match object; span=(0, 55), match='{\"name\": \"list_files\", \"arguments\": {\"directory\":> {\"name\": \"list_files\", \"arguments\": {\"directory\": \"/\"}}}dlk\n",
      "checking directory /\n",
      "output:bin, boot, dev, etc, home, lib, lib32, lib64, libx32, media, mnt, opt, proc, root, run, sbin, srv, sys, tmp, usr, var, .dockerenv, workspace, .singularity.d\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "def list_files(directory):\n",
    "    \"\"\"\n",
    "    Lists files in the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to the directory\n",
    "    \n",
    "    Returns:\n",
    "        list: List of files in the directory\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print('checking directory',directory)\n",
    "        return ', '.join(os.listdir(directory))\n",
    "    except Exception as e:\n",
    "        return f\"Error listing files: {e}\"\n",
    "def clean_json(json_str):\n",
    "    \"\"\"\n",
    "    Clean the JSON string by removing trailing characters that might break parsing.\n",
    "    \n",
    "    Args:\n",
    "        json_str (str): The potentially malformed JSON string\n",
    "    \n",
    "    Returns:\n",
    "        str: A cleaned JSON string that can be parsed by json.loads()\n",
    "    \"\"\"\n",
    "    # Remove the 'function_call:' prefix and any surrounding text\n",
    "    json_str = re.sub(r'^.*?function_call:\\s*', '', json_str, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove escaped quotes and unescape the string\n",
    "    json_str = json_str.replace('\\\\\"', '\"').strip()\n",
    "    \n",
    "    # Remove any trailing characters after the JSON object\n",
    "    json_str = re.sub(r'}.*$', '}', json_str, flags=re.DOTALL)\n",
    "    return json_str\n",
    "\n",
    "def execute_function_call(llm_output):\n",
    "    \"\"\"\n",
    "    Extracts and executes the function call from LLM output.\n",
    "    \n",
    "    Args:\n",
    "        llm_output (str): The full LLM output string\n",
    "    \n",
    "    Returns:\n",
    "        Result of the function call or None if no valid function call\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use regex to find the JSON between function_call: and the next closing brace\n",
    "    llm_output = llm_output.replace('parameters','arguments')\n",
    "    match = re.search(r'\\s*({.*})', llm_output, re.DOTALL)\n",
    "    match = re.search(r'\\s*({(?:\\\\}|[^}])*?}?})(?:,|\\]|\\'|\\\")?', llm_output, re.DOTALL)\n",
    "    print('match',match, llm_output)\n",
    "    if match:\n",
    "        try:\n",
    "            # Extract the potential JSON string\n",
    "            potential_json = match.group(1)\n",
    "            # Clean the JSON string\n",
    "            cleaned_json = clean_json(potential_json)\n",
    "            # Parse the cleaned JSON\n",
    "            function_call = json.loads(potential_json)\n",
    "           \n",
    "            \n",
    "            # Get the function name and arguments\n",
    "            func_name = function_call.get('name')\n",
    "            args = function_call.get('arguments', {})\n",
    "\n",
    "            \n",
    "            # Dynamically call the function\n",
    "            if func_name == 'list_files':\n",
    "                return 'output:'+ list_files(args.get('directory'))\n",
    "            if func_name == 'get_status':\n",
    "                return 'output:'+get_status()\n",
    "            else:\n",
    "                return f\"Unknown function: {func_name}\"\n",
    "        \n",
    "        except json.JSONDecodeError as e:\n",
    "            return f\"JSON Parsing Error: {e}. Original string: {potential_json}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error processing function call: {e}\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "sample_output = '{\"name\": \"list_files\", \"parameters\": {\"directory\": \"/\"}}}dlk'\n",
    "result = execute_function_call(sample_output)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27379e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89646582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def react():\n",
    "    global react_agent, cleaner_agent, rephrase_agent,  prompt, system_message, tools\n",
    "    READER_MODEL_NAME1 = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "    READER_MODEL_NAME2 = \"tiiuae/falcon-7b-instruct\"\n",
    "    READER_MODEL_NAME3 = 'teknium/OpenHermes-2.5-Mistral-7B'\n",
    "    READER_MODEL_NAME4 = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "    READER_MODEL_NAME5 = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "    READER_MODEL_NAME6 = \"meta-llama/Llama-3.1-8B\"\n",
    "    READER_MODEL_NAME7 = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    READER_MODEL_NAME8 = \"meta-llama/Meta-Llama-3.1-8b-Instruct\"\n",
    "    READER_MODEL_NAME9 = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "    \n",
    "    if react_agent is None:\n",
    "        react_agent = Agent(READER_MODEL_NAME8,\"react\")\n",
    "        cleaner_agent = Agent(READER_MODEL_NAME9,'cleaner')\n",
    "        #rephrase_agent = Agent(READER_MODEL_NAME6,'rephrase')\n",
    "        \n",
    "    react_agent.system_message = system_message\n",
    "    react_agent.tools = tools\n",
    "    #thedir = react_agent.list_files_in_directory(\"/\")\n",
    "    \n",
    "    question = prompt\n",
    "    agent_response = react_agent.llm_generate_response(question)\n",
    "    print('####################################')\n",
    "    agent_response = list(agent_response.split(['assistant'][-1]))[-1]\n",
    "    print(agent_response)\n",
    "    print('####################################')\n",
    "    \n",
    "    cleaner_prompt=[{\"role\":\"system\",\"content\":\"You are smart text understanding expert, Extract the results from the given prompt\\\n",
    "     Ignore the texts that has not meaning and just extract the results\"},\n",
    "                   {\"role\":\"user\",\"content\":agent_response}]\n",
    "    filter_prompt = f\"\"\"\n",
    "You are a precise extraction of assistant: or Assistant: replies.\\\n",
    "Your ONLY task is to find and return the result from the given text. \n",
    "\n",
    "Rules:\n",
    "- Look for the single, precise explanation output\n",
    "- if it is numerical  reply only with the numerical\n",
    "- Ignore all surrounding text or context\n",
    "- If no clear numerical result is found, respond with the found explanation\n",
    "- If you find a list of dictionnaries or a list of json data after the assistant/Assistant words, \\\n",
    "then say the word 'Execute Functions:' and list them\n",
    "- Extract all the explanation if it shows that there is no results or otherwise, extract ONLY the numerical value\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    cleaner_prompt = [{\"role\":\"system\",\"content\":filter_prompt}, {\"role\":\"user\",\"content\":agent_response}]\n",
    "    #cleaner_response = cleaner_agent.generate_response(cleaner_prompt)\n",
    "    #cleaned_response = cleaner_agent.clear_response(cleaner_prompt, cleaner_response)\n",
    "    try:\n",
    "        #print(';;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;')\n",
    "        #print('agent_resposne:',type(agent_response),str(agent_response.split(['Assistant:'][-1])))\n",
    "        #print(';;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;')\n",
    "        #print('cleaner_response:',cleaner_response)\n",
    "        print(';;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;')\n",
    "        #print('cleaned_response:',cleaned_response)\n",
    "        print(';;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;')\n",
    "        function_call = execute_function_call(agent_response)\n",
    "        if 'output:' in function_call:\n",
    "            #prompt = 'the function is called and the output is:' + function_call + 'so please \\\n",
    "            #rephrase youre reply'\n",
    "            rephrase_filter = \" you are expert in english language you rephrase any text you professionally.\\\n",
    "                    be very brief and reply by rephrasing the given text and starty\"\n",
    "            \n",
    "            rephrase_text = \"rephrase in a bief way and do not add any infomration from your side \\\n",
    "            : After cheking the outside environment, the  reply is:\"+function_call\n",
    "            cleaner_prompt = [{\"role\":\"system\",\"content\":rephrase_filter}, {\"role\":\"user\",\"content\":rephrase_text}]\n",
    "            print('hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh')\n",
    "            rephrase_response = cleaner_agent.generate_response(cleaner_prompt)\n",
    "            cleaned_response = cleaner_agent.clear_response(cleaner_prompt, rephrase_response)\n",
    "            print(cleaned_response)\n",
    "            \n",
    "            \n",
    "    except:\n",
    "        print('all is not running')\n",
    "      \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ada0a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b86342bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['torch_dtype', 'device_map', 'bnb_4bit_quantw_type', 'use_nested_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load the mode: meta-llama_Meta-Llama-3.1-8b-Instruct_saved_response from local repo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b35f4051c943779767d4a99294e1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['torch_dtype', 'device_map', 'bnb_4bit_quantw_type', 'use_nested_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load the mode: meta-llama_Llama-3.2-1B-Instruct_saved_response from local repo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################\n",
      "\n",
      "\n",
      "Assistant: {\"name\": \"list_files\", \"parameters\": {\"directory\": \"/\"}}\n",
      "####################################\n",
      ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
      ";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n",
      "match <re.Match object; span=(12, 68), match=' {\"name\": \"list_files\", \"arguments\": {\"directory\"> \n",
      "\n",
      "Assistant: {\"name\": \"list_files\", \"arguments\": {\"directory\": \"/\"}}\n",
      "checking directory /\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "\n",
      "The command to check the outside environment is: \n",
      "```bash\n",
      "bin, boot, dev, etc, home, lib, lib32, lib64, libx32, media, mnt, opt, proc, root, run, sbin, srv, sys, tmp, usr, var, dockerenv, workspace, singularity.d\n",
      "```\n",
      "\n",
      "Explanation:\n",
      "- bin: Binary files\n",
      "- boot: Boot loader\n",
      "- dev: Device files\n",
      "- etc: System information\n",
      "- home: Home directory\n",
      "- lib: Library files\n",
      "- lib32: Library files (32-bit)\n",
      "- lib64: Library files (64-bit)\n",
      "- libx32: Library files (32-bit)\n",
      "- media: Media devices\n",
      "- mnt: Mount point\n",
      "- opt: Option files\n",
      "- proc: Process information\n",
      "- root: Root directory\n",
      "- run: Run directory\n",
      "- sbin: System binary files\n",
      "- srv: Service files\n",
      "- sys: System information\n",
      "- tmp: Temporary files\n",
      "- usr: \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tools=[]\n",
    "    tools = [get_status, list_files]\n",
    "    system_message = \"you are a bot .\\\n",
    "    start youre reply with Assistant:. if the question needs you to run a function, then only say the following:\\\n",
    "    function_call: and add the function and arguments in a json format and end your reply. \\\n",
    "    remember not to state any other function that are not needed.\\\n",
    "    and remember to not to run the function. \\\n",
    "    if there is no function is needed to call, then be brief in your reply and only reply directly\\\n",
    "  \"\n",
    "    prompt = \"list the files in the / \"\n",
    "    #prompt = \"get status\"\n",
    "    #prompt = \"what files found in the /bin ?\"\n",
    "    react()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c50ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fb5028",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
