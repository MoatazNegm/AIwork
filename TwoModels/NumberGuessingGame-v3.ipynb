{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1651da74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install langgraph\n",
    "#!pip3  install torch torchvision torchaudio transformers\n",
    "#!pip3 install packaging ninja\n",
    "#!pip3 install accelerate\n",
    "#!pip3 install protobuf\n",
    "#!pip3 install sentencepiece\n",
    "#!pip3 install bitsandbytes\n",
    "#!pip3 install scipy\n",
    "#!pip3 install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f71f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f6f9cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "playgame = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53dad680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, LlamaTokenizer, LlamaForCausalLM, MistralForCausalLM\n",
    "import random, json\n",
    "import inspect\n",
    "import json\n",
    "from typing import Dict, Any, Optional\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c28284f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_setter = None\n",
    "guesser = None\n",
    "react_agent = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c3a75b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Agent\n",
    "class GuessingGame:\n",
    "    def __init__(self,low_limit,high_limit, modelnumber1=1,modelnumber2=1):\n",
    "        self.low_limit = low_limit\n",
    "        self.high_limit = high_limit\n",
    "        self.agent1_name  = 'Player1'\n",
    "        self.agent2_name  = 'Player2'\n",
    "        self.modelnumber1 = modelnumber1\n",
    "        self.modelnumber2 = modelnumber2\n",
    "        self.target = ''\n",
    "        self.messsages = ''\n",
    "        self.agent1_messages = \"You are Agent1 in a gussing play. \\\n",
    "        You compare tow numbers: number1 is NUMBER and number2 you get from the user. \\\n",
    "        You reply very briefly to guide the user to guess a higher or lower number to reach NUMBER or\\\n",
    "        to say it is Correct if the user guesses NUMBER\"\n",
    "        self.agent2_messages =\"You are Agent2 in a gussing play. \\\n",
    "        you guess an integer number. this number is between HIGH and LOW and should be not one of the numbers that the user will provide you.\\\n",
    "        Your guess should be according to the user user instruction.and be very brief in your reply \"\n",
    "        self.agent1 = \"\"\n",
    "        self.agent2 = \"\"\n",
    "        \n",
    "    def set_target_number(self):\n",
    "        self.target_number = random.randint(self.low_limit, self.high_limit)\n",
    "        print(f\"{self.agent1_name} has set a secret number.{self.target_number}\")\n",
    "        \n",
    "        return self.target_number\n",
    "\n",
    "    def check_guess(self, guess):\n",
    "        agent1_prompt = guess\n",
    "        self.messages = self.agent1_messages + [{\"role\": \"user\", \"content\": agent1_prompt}]\n",
    "        checking = self.agent1.generate_response(self.messages)\n",
    "        #print('agent1_prompt',agent1_prompt)\n",
    "        #print('checking',checking)\n",
    "        return checking\n",
    "\n",
    "    def play_game(self):\n",
    "        # Set target number\n",
    "        self.target = self.set_target_number()\n",
    "        guesses = []\n",
    "        attempts = 0\n",
    "        if not self.agent1:\n",
    "            self.agent1_messages = self.agent1_messages.replace('NUMBER',str(self.target))\n",
    "            agent1_response,self.agent1 = Agent.agentthis(\"\",self.agent1_messages,self.modelnumber1)\n",
    "            print(f\"{self.agent1_name}: {agent1_response}\")\n",
    "            \n",
    "            self.agent2_messages = self.agent2_messages.replace('HIGH',str(self.high_limit)).replace('LOW',str(self.low_limit))\n",
    "            agent2_response,self.agent2 = Agent.agentthis(\"\",self.agent2_messages,self.modelnumber2)\n",
    "            print(f\"{self.agent2_name}: {agent2_response}\")\n",
    "        \n",
    "        init_prompt = f\"please guess the number\"\n",
    "        guess_response = self.agent2.generate_response(init_prompt)\n",
    "       \n",
    "        while attempts < 10:  # Limit attempts\n",
    "           \n",
    "            attempts += 1\n",
    "            guesses.append(guess_response)\n",
    "            print(f\"{self.agent2_name} guesses: {guess_response}\")\n",
    "            # Check guess\n",
    "            check_response = self.agent1.generate_response(guess_response+', checkthis against your secret number')\n",
    "            #agent_response_loop = self.check_guess(agent1_response)\n",
    "            print(f\"{self.agent1_name} says: {check_response}\")\n",
    "            \n",
    "            # Check if correct\n",
    "            if 'Correct' in str(check_response) or 'correct' in str(check_response):\n",
    "                print ('agent1 completes',check_response)\n",
    "                print(f\"Game won in {attempts + 1} attempts!\")\n",
    "                break\n",
    "            else:\n",
    "                print(check_response)\n",
    "            # Generate guess prompt\n",
    "           \n",
    "            # Get guess from other agent\n",
    "            guess_prompt = f\"Please,  guess again and it should not be any number in this list {guesses}\\\n",
    "            but it must be between {self.high_limit} and {self.low_limit}\"\n",
    "            guess_response = self.agent2.generate_response(guess_prompt)\n",
    "            \n",
    "        else:\n",
    "            print(f\"Game over. The number was {self.target}\")\n",
    "\n",
    "# Note: This is a conceptual implementation\n",
    "# Actual usage requires Qwen model installation and proper setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb5e5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89646582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global playgame\n",
    "    if playgame is None:\n",
    "        playgame = GuessingGame(1,30, 1,8)\n",
    "    playgame.history = 1\n",
    "    playgame.play_game()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b86342bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['torch_dtype', 'device_map', 'bnb_4bit_quantw_type', 'use_nested_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:174: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player1 has set a secret number.12\n",
      "Trying to load the mode: Qwen_Qwen2.5-Coder-7B-Instruct_saved_response from local repo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e6abae53844c89a4b21a0c3d600f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=500) and `max_length`(=8500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Unused kwargs: ['torch_dtype', 'device_map', 'bnb_4bit_quantw_type', 'use_nested_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player1:  Yes, I'm ready! Let's start guessing. Think of a number between 1 and 20.\n",
      "Trying to load the mode: meta-llama_Meta-Llama-3.1-8b-Instruct_saved_response from local repo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f7696cda284b0b9a27feca945afaae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Both `max_new_tokens` (=500) and `max_length`(=8500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Both `max_new_tokens` (=500) and `max_length`(=8500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player2:  Yes, I'm ready. What's the first hint? \n",
      "\n",
      "(Also, just to confirm, the number I'm thinking of is between 30 and 1, and you can't guess any of the numbers I'll give you as hints, right?) \n",
      "Please provide the first hint. \n",
      "Please give me a hint.  I'll start guessing. \n",
      "Guess: 16. \n",
      "Is 16 too high or too low? \n",
      "Please provide the next hint. \n",
      "Please give me another hint. \n",
      "Guess: 10. \n",
      "Is 10 too high or too low? \n",
      "Please give me another hint. \n",
      "Guess: 7. \n",
      "Is 7 too high or too low? \n",
      "Please give me another hint. \n",
      "Guess: 5. \n",
      "Is 5 too high or too low? \n",
      "Please give me another hint. \n",
      "Guess: 3. \n",
      "Is 3 too high or too low? \n",
      "Please give me another hint. \n",
      "Guess: 2. \n",
      "Is 2 too high or too low? \n",
      "Please give me another hint. \n",
      "Guess: 1. \n",
      "Is 1 too high or too low? \n",
      "Please give me another hint. \n",
      "Guess: 4. \n",
      "Is 4 too high or too low? \n",
      "Please give me another hint. \n",
      "Guess: 9. \n",
      "Is 9 too high or too low? \n",
      "Please give me another hint. \n",
      "Guess: 6. \n",
      "Is 6 too high or too low? \n",
      "Please give me another hint. \n",
      "Guess: 8. \n",
      "Is 8 too high or too low? \n",
      "Please give me another hint. \n",
      "Guess: 12. \n",
      "Is 12 too high or too low? \n",
      "Please give me another hint. \n",
      "Guess: 15. \n",
      "Is 15 too high or too low? \n",
      "Please give me another hint. \n",
      "Guess: 11. \n",
      "Is 11 too high or too low? \n",
      "Please give me another hint. \n",
      "Guess: 14. \n",
      "Is 14 too high or too low? \n",
      "Please give me another hint. \n",
      "Guess: 13. \n",
      "Is 13 too high or too low? \n",
      "Please give me another hint. \n",
      "Guess: 17. \n",
      "Is 17 too high or too low? \n",
      "Please give me another hint. \n",
      "Guess: 18. \n",
      "Is 18 too high or too low? \n",
      "Please give me another hint. \n",
      "Guess: 19. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=500) and `max_length`(=8500) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player2 guesses:  My next guess is 31. Is\n",
      "Player1 says:  Yes, that's correct! Well done!\n",
      "agent1 completes  Yes, that's correct! Well done!\n",
      "Game won in 2 attempts!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c21d90b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are Agent1 in a gussing play.         You compare tow numbers: number1 is 12 and number2 you get from the user.         You reply very briefly to guide the user to guess a higher or lower number to reach 12 or        to say it is Correct if the user guesses 12'},\n",
       " {'role': 'user', 'content': 'Are you ready ?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \" Yes, I'm ready! Let's start guessing. Think of a number between 1 and 20.\"},\n",
       " {'role': 'user',\n",
       "  'content': ' My next guess is 31. Is, checkthis against your secret number'},\n",
       " {'role': 'assistant', 'content': \" Yes, that's correct! Well done!\"}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playgame.agent1.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b9d09fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: prompt, system_message, modelsel\n",
      "modelsel can be a number to select the following:\n",
      "modelsel = 1: Qwen/Qwen2.5-Coder-7B-Instruct\n",
      "modelsel = 2: tiiuae/falcon-7b-instruct\n",
      "modelsel = 3: teknium/OpenHermes-2.5-Mistral-7B\n",
      "modelsel = 4: meta-llama/Llama-3.2-3B-Instruct\n",
      "modelsel = 5: mistralai/Mistral-7B-Instruct-v0.3\n",
      "modelsel = 6: meta-llama/Llama-3.1-8B\n",
      "modelsel = 7: meta-llama/Llama-3.1-8B-Instruct\n",
      "modelsel = 8: meta-llama/Meta-Llama-3.1-8b-Instruct\n",
      "modelsel = 9: meta-llama/Llama-3.2-1B-Instruct\n",
      "modelsel = 10: mistralai/Mixtral-8x22B-Instruct-v0.1\n"
     ]
    }
   ],
   "source": [
    "Agent.agentthis('help')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b5f08173",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6232dff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3677606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
