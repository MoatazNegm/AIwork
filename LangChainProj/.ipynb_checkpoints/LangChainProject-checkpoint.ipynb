{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YIs4bngsPm8"
   },
   "source": [
    "## <font color=blue> TASK 1: Install and setup LangChain\n",
    "Welcome to this project notebook, which will serve as your guide to constructing your inaugural Generative AI application. Within this notebook, you'll encounter concise descriptions of each task to enhance your comprehension of the sequence. Our initial task commences with the importation of essential libraries required for this project.\n",
    "\n",
    "***Note: Before importing the libraries please ensure that all the library modules such as Langchain, Streamlit, PyPdf and other required libraries are installed on this notebook by running the pip command as shown below.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wnvxw5okxYjT",
    "outputId": "fadee241-7a02-4775-c4b1-0b1857d4ce8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.2.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.30-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Downloading dataclasses_json-0.6.6-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain)\n",
      "  Downloading langchain_core-0.2.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.1.59-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain)\n",
      "  Downloading tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading marshmallow-3.21.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (23.2)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
      "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain)\n",
      "  Downloading jsonpointer-2.4-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain-0.2.0-py3-none-any.whl (973 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.7/973.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
      "Downloading langchain_core-0.2.0-py3-none-any.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.9/307.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
      "Downloading langsmith-0.1.59-py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.2/121.2 kB\u001b[0m \u001b[31m969.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.30-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
      "Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.0/616.0 kB\u001b[0m \u001b[31m585.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m708.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m272.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: tenacity, orjson, mypy-extensions, marshmallow, jsonpointer, greenlet, typing-inspect, SQLAlchemy, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain\n",
      "Successfully installed SQLAlchemy-2.0.30 dataclasses-json-0.6.6 greenlet-3.0.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.2.0 langchain-core-0.2.0 langchain-text-splitters-0.2.0 langsmith-0.1.59 marshmallow-3.21.2 mypy-extensions-1.0.0 orjson-3.10.3 tenacity-8.3.0 typing-inspect-0.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting langchain_openai\n",
      "  Downloading langchain_openai-0.1.7-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.46 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.2.0)\n",
      "Collecting openai<2.0.0,>=1.24.0 (from langchain_openai)\n",
      "  Downloading openai-1.30.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.46->langchain_openai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.46->langchain_openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.46->langchain_openai) (0.1.59)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.46->langchain_openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.46->langchain_openai) (2.6.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.46->langchain_openai) (8.3.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting anyio<5,>=3.5.0 (from openai<2.0.0,>=1.24.0->langchain_openai)\n",
      "  Downloading anyio-4.3.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.24.0->langchain_openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.24.0->langchain_openai)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting sniffio (from openai<2.0.0,>=1.24.0->langchain_openai)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.24.0->langchain_openai) (4.10.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain_openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain_openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai)\n",
      "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain_openai)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.46->langchain_openai) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.46->langchain_openai) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.46->langchain_openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.46->langchain_openai) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (1.26.18)\n",
      "Downloading langchain_openai-0.1.7-py3-none-any.whl (34 kB)\n",
      "Downloading openai-1.30.1-py3-none-any.whl (320 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sniffio, h11, distro, tiktoken, httpcore, anyio, httpx, openai, langchain_openai\n",
      "Successfully installed anyio-4.3.0 distro-1.9.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 langchain_openai-0.1.7 openai-1.30.1 sniffio-1.3.1 tiktoken-0.7.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.10.0)\n",
      "Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m283.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf\n",
      "Successfully installed pypdf-4.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting streamlit\n",
      "  Downloading streamlit-1.34.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.3.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Downloading blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.24.4)\n",
      "Requirement already satisfied: packaging<25,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.2)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.5.3)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.2.0)\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.24.4)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.3.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.10.0)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.4)\n",
      "Collecting watchdog>=2.1.5 (from streamlit)\n",
      "  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl.metadata (37 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.21.1)\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.17.2)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.34.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
      "Downloading streamlit-1.34.0-py2.py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/8.5 MB\u001b[0m \u001b[31m186.9 kB/s\u001b[0m eta \u001b[36m0:00:38\u001b[0m"
     ]
    }
   ],
   "source": [
    "#For installing the Langchain module associated with OpenAI LLM model\n",
    "!pip install langchain\n",
    "!pip install langchain_openai\n",
    "#For installing the Python library responsible for PDF upload\n",
    "!pip install pypdf\n",
    "#For installing the library responsible for web app development\n",
    "!pip install streamlit\n",
    "#For installing tokeniser library that asists with converting text strings into tokens recognizable by OpenAI models\n",
    "!pip install tiktoken\n",
    "#For installing the library used for invoking the environment file containing secret API key\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bJ0AR0gorTlF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvtUejaS8v8T"
   },
   "source": [
    "## <font color=blue> Load OpenAI API Key to access LLM model\n",
    "\n",
    "## <font color=black>\n",
    "1. Go to https://platform.openai.com/api-keys\n",
    "2. Click on the '+ Creat new secret key button'\n",
    "3. Enter an identifier name(optional) and click on the \"Create secret key\" button\n",
    "4. Copy the API key to be used in the API.env file that you need to upload to Google Colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qtlBVZonvkf8"
   },
   "outputs": [],
   "source": [
    "# Load the .env file and invoke the secret API key from the file\n",
    "dotenv.load_dotenv('API.env')\n",
    "OpenAI.api_key = os.getenv(\"OPEN_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRjptllP7Xgb"
   },
   "source": [
    "# <font color=blue> Load PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FL4FtXMkvocT"
   },
   "outputs": [],
   "source": [
    "pdf_url = \"https://www.medrxiv.org/content/10.1101/2021.07.15.21260605v1.full.pdf\"\n",
    "\n",
    "loader =\n",
    "pages =\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HEGnGRAe_op2"
   },
   "outputs": [],
   "source": [
    "#number of pages\n",
    "len(pages)\n",
    "\n",
    "#view page content\n",
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyDp0eJj_hNd"
   },
   "source": [
    "## <font color=blue> TASK 2: Define the summarize pdf function\n",
    "Define the main function that will take pdf file path as an input and generate a summary of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kON5tJmk09Mp"
   },
   "outputs": [],
   "source": [
    "def summarize_pdf(pdf_file_path, chunk_size, chunk_overlap):\n",
    "\n",
    "    #Instantiate LLM model gpt-3.5-turbo-16k\n",
    "\n",
    "    #Load PDF file\n",
    "\n",
    "\n",
    "    #Create multiple documents\n",
    "\n",
    "\n",
    "    #Split text into chunks\n",
    "\n",
    "\n",
    "    #Summarize the chunks (map_reduce method takes longer to execute)\n",
    "\n",
    "\n",
    "    #Return the summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jP624HFsZyT"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zZsZ5TGMM-WS"
   },
   "outputs": [],
   "source": [
    "#print summary by using chain type stuff or map_reduce\n",
    "#Chunk size and chunk overlap values set to random value\n",
    "\n",
    "print(summarize_pdf(pdf_url, 1000, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cvnt8H-L3Yzb"
   },
   "source": [
    "## <font color=blue> TASK 3: Add Prompt template to the summarizer function\n",
    "Leveraging prompt templates to extract key information from the reserach paper in more guided manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_s2a87sV4Hkl"
   },
   "source": [
    "## <font color=blue> Define Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Wy1_iJlfQRR"
   },
   "source": [
    "### <font color=black> Prompt Template for Stuffing chain type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Zac7tS6p3YKU"
   },
   "outputs": [],
   "source": [
    "map_prompt_template = \"\"\"\n",
    "                       Write a summary of the research paper for an\n",
    "                       artficial intelligence researcher that includes\n",
    "                       main points and any important details in bullet points.{text}\n",
    "                      \"\"\"\n",
    "\n",
    "map_prompt ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Il7HTR6pfjsF"
   },
   "source": [
    "### <font color=black> Add Combo Template for Map_Reduce chain type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "AaZoSIMDfMPu"
   },
   "outputs": [],
   "source": [
    "combine_prompt_template =\n",
    "\n",
    "combine_prompt ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6xyYdjJZEJJs"
   },
   "outputs": [],
   "source": [
    "#Modify the custom function to add the prompt templates\n",
    "\n",
    "def summarize_pdf(pdf_file_path, chunk_size, chunk_overlap):\n",
    "\n",
    "    #Instantiate LLM model gpt-3.5-turbo-16k\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0, openai_api_key=OpenAI.api_key)\n",
    "\n",
    "    #Load PDF file\n",
    "    loader = PyPDFLoader(pdf_file_path)\n",
    "    docs_raw = loader.load()\n",
    "\n",
    "    #Create multiple documents\n",
    "    docs_raw_text = [doc.page_content for doc in docs_raw]\n",
    "\n",
    "    #Split text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    docs_chunks = text_splitter.create_documents(docs_raw_text)\n",
    "\n",
    "    #Summarize the chunks\n",
    "    chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "\n",
    "    #Return the summary\n",
    "    summary = chain.invoke(docs_chunks, return_only_outputs=True)\n",
    "    return summary['output_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_2bNEpmcPG0w"
   },
   "outputs": [],
   "source": [
    "#print summary using the map_prompt and combine prompt\n",
    "#Increasing the chunk size value might reduce the overall summarization time with map_reduce method\n",
    "print(summarize_pdf(pdf_url, 2000, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XUnEVtU849_"
   },
   "source": [
    "## <font color=blue> TASK 4: Build and test a GenAI app for PDF summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "urhXeIwaS6RN"
   },
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import streamlit as st\n",
    "\n",
    "\n",
    "# Load the .env file and invoke the secret API key from the file\n",
    "dotenv.load_dotenv('API.env')\n",
    "OpenAI.api_key = os.getenv(\"OPEN_API_KEY\")\n",
    "\n",
    "#summarize_pdf function\n",
    "\n",
    "def summarize_pdf(pdf_file_path, chunk_size, chunk_overlap, prompt):\n",
    "    #Instantiate LLM model gpt-3.5-turbo-16k\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0, openai_api_key=OpenAI.api_key)\n",
    "\n",
    "    #Load PDF file\n",
    "    loader = PyPDFLoader(pdf_file_path)\n",
    "    docs_raw = loader.load()\n",
    "\n",
    "    #Create multiple documents\n",
    "    docs_raw_text = [doc.page_content for doc in docs_raw]\n",
    "\n",
    "    #Split text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    docs_chunks = text_splitter.create_documents(docs_raw_text)\n",
    "\n",
    "    #Summarize the chunks\n",
    "    chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt = prompt)\n",
    "    #Return the summary\n",
    "    summary = chain.invoke(docs_chunks, return_only_outputs=True)\n",
    "    return summary['output_text']\n",
    "\n",
    "#streamlit app main() function\n",
    "\n",
    "def main():\n",
    "    #Set page config and title\n",
    "\n",
    "\n",
    "    #Input pdf file path\n",
    "\n",
    "\n",
    "    #prompt input\n",
    "\n",
    "    #Summarize button\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10C8V-CIL4Ti"
   },
   "source": [
    "## <font color=blue> Launch Streamlit app from Google Colab\n",
    "\n",
    "The following lines of code would enable users to launch Streamlit app from Google Colab using [ngrok service](https://ngrok.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jaPE0T2iDzwu"
   },
   "outputs": [],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Brgmtoc7CVKx"
   },
   "outputs": [],
   "source": [
    "!unzip ngrok-stable-linux-amd64.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "hle-PIMoQfLs"
   },
   "outputs": [],
   "source": [
    "get_ipython().system_raw('./ngrok http 8501 &')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7C6dydMS06y"
   },
   "outputs": [],
   "source": [
    "!wget -q -O - ipv4.icanhazip.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1Rduaz0TG-Y"
   },
   "outputs": [],
   "source": [
    "!streamlit run app.py & npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJtA0iPfgBn_"
   },
   "source": [
    "## <font color=blue> FINAL TASK: Cumulative Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFTCcg_COEie"
   },
   "source": [
    "Click the link to explore useful Streamlit library functions:\n",
    "https://docs.streamlit.io/library/cheatsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PiEdC80-gP-k"
   },
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "import streamlit as st\n",
    "\n",
    "# Load the .env file and invoke the secret API key from the file\n",
    "dotenv.load_dotenv('API.env')\n",
    "OpenAI.api_key = os.getenv(\"OPEN_API_KEY\")\n",
    "\n",
    "#summarize_pdf function\n",
    "\n",
    "def summarize_pdf(pdf_file_path, chunk_size, chunk_overlap, chain_type, prompt):\n",
    "    #Instantiate LLM model gpt-3.5-turbo-16k\n",
    "    llm=ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0, openai_api_key=OpenAI.api_key)\n",
    "\n",
    "    #Load PDF file\n",
    "    loader = PyPDFLoader(pdf_file_path)\n",
    "    docs_raw = loader.load()\n",
    "\n",
    "    #Create multiple documents\n",
    "    docs_raw_text = [doc.page_content for doc in docs_raw]\n",
    "\n",
    "    #Split text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    docs_chunks = text_splitter.create_documents(docs_raw_text)\n",
    "\n",
    "    #Create multiple prompts\n",
    "\n",
    "\n",
    "    #Summarize the chunks\n",
    "\n",
    "    #Return the summary\n",
    "    return chain.run(docs_chunks)\n",
    "\n",
    "#streamlit app main() function\n",
    "\n",
    "def main():\n",
    "    #Set page config and title\n",
    "    st.set_page_config(page_title=\"PDF Summarizer\", page_icon=\":book:\", layout=\"wide\")\n",
    "    st.title(\" \")\n",
    "\n",
    "    #Add custom sliders and selectbox for more user interaction\n",
    "\n",
    "\n",
    "    #Display warning message\n",
    "\n",
    "\n",
    "    #Input pdf file path\n",
    "    pdf_file_path = st.text_input(\"Enter PDF file path:\")\n",
    "\n",
    "    #Prompt input\n",
    "    user_prompt = st.text_input(\"Enter prompt:\")\n",
    "\n",
    "    #Summarize button\n",
    "    if st.button(\"Summarize\"):\n",
    "        #Summarize pdf\n",
    "        summary = summarize_pdf(pdf_file_path, chunk_size, chunk_overlap, chain_type, user_prompt)\n",
    "        st.write(summary)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xn7-oPQOPNUD"
   },
   "outputs": [],
   "source": [
    "!wget -q -O - ipv4.icanhazip.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "141g6WFTPVbM"
   },
   "outputs": [],
   "source": [
    "!streamlit run app.py & npx localtunnel --port 8501"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
